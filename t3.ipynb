{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task C: Train RNN on FSDD; quantized; weights in pow2\n",
    "# - same as Task B, custom quant fcn for params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch.quantization as quantization\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sigproc import gen_logmel, feat2img\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mainly adapted from https://github.com/saztorralba/CNNWordReco due to following\n",
    "# - deeplake / hub version broken -> replaced with original wavs (cloned orig repo: https://github.com/Jakobovski/free-spoken-digit-dataset)\n",
    "# - logmel suitable for detection of spoken speech -> normalized, resampled, high-pass filtered, time axis scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'train_val_percentage': 0.1,\n",
    "    'xsize': 20,\n",
    "    'ysize': 20,\n",
    "    'rnn_layers': 3,\n",
    "    'rnn_hidden': 64,\n",
    "    'rnn_outputs': 10,\n",
    "    'epochs': 1000,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "    'device': 'cpu',\n",
    "    'verbose': 1,\n",
    "    'augment': False,\n",
    "    'vocab': OrderedDict({'ZERO': 0, 'ONE': 1, 'TWO': 2, 'THREE': 3, 'FOUR': 4, 'FIVE': 5, 'SIX': 6, 'SEVEN': 7, 'EIGHT': 8, 'NINE': 9})\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## labels and paths in pd frame\n",
    "wavfiles = glob.glob('./free-spoken-digit-dataset/recordings/*.wav')\n",
    "speakers = [file.split('/')[-1].split('_')[1] for file in wavfiles]\n",
    "words = [list(args['vocab'].keys())[int(file.split('/')[-1].split('_')[0])] for file in wavfiles]\n",
    "rec_number = [int(file.split('/')[-1].split('_')[2].split('.')[0]) for file in wavfiles]\n",
    "data = pd.DataFrame({'wavfile':wavfiles,'speaker':speakers,'word':words,'rec_number':rec_number})\n",
    "\n",
    "## train/test split according to https://github.com/Jakobovski/free-spoken-digit-dataset\n",
    "train_data = data.loc[data['rec_number']>=5].reset_index(drop=True)\n",
    "test_data = data.loc[data['rec_number']<5].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log mels for audio; time scaled by PIL.Image to xsize, 40 nmels\n",
    "def load_data(data,cv=False,**kwargs):\n",
    "    n_samples = len(data)\n",
    "    dataset = torch.zeros((n_samples,kwargs['ysize'],kwargs['xsize']),dtype=torch.uint8)\n",
    "    labels = torch.zeros((n_samples),dtype=torch.uint8)\n",
    "    for i in tqdm(range(n_samples),disable=(kwargs['verbose']<2)):\n",
    "        path = data['wavfile'][i]\n",
    "        dataset[i,:,:] = torch.from_numpy(feat2img(gen_logmel(path,(kwargs['n_mels'] if 'n_mels' in kwargs else 40),(kwargs['sampling'] if 'sampling' in kwargs else 8000),True),kwargs['ysize'],kwargs['xsize']))\n",
    "        labels[i] = kwargs['vocab'][data['word'][i]]\n",
    "\n",
    "    if cv == False:\n",
    "        return dataset, labels\n",
    "\n",
    "    #Do random train/validation split\n",
    "    idx = [i for i in range(n_samples)]\n",
    "    random.shuffle(idx)\n",
    "    trainset = dataset[idx[0:int(n_samples*(1-kwargs['train_val_percentage']))]]\n",
    "    trainlabels = labels[idx[0:int(n_samples*(1-kwargs['train_val_percentage']))]]\n",
    "    validset = dataset[idx[int(n_samples*(1-kwargs['train_val_percentage'])):]]\n",
    "    validlabels = labels[idx[int(n_samples*(1-kwargs['train_val_percentage'])):]]\n",
    "    return trainset, validset, trainlabels, validlabels\n",
    "\n",
    "def load_test_data(data,**kwargs):\n",
    "    n_samples = len(data)\n",
    "    dataset = torch.zeros((n_samples,kwargs['ysize'],kwargs['xsize']),dtype=torch.uint8)\n",
    "    labels = torch.zeros((n_samples),dtype=torch.uint8)\n",
    "    for i in tqdm(range(n_samples),disable=(kwargs['verbose']<2)):\n",
    "        path = data['wavfile'][i]\n",
    "        dataset[i,:,:] = torch.from_numpy(feat2img(gen_logmel(path,(kwargs['n_mels'] if 'n_mels' in kwargs else 40),(kwargs['sampling'] if 'sampling' in kwargs else 8000),True),kwargs['ysize'],kwargs['xsize']))\n",
    "        labels[i] = kwargs['vocab'][data['word'][i]]\n",
    "\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2430, 20, 20]) torch.Size([270, 20, 20])\n",
      "torch.Size([300, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "trainset, validset, trainlabels, validlabels = load_data(train_data,True,**args)\n",
    "print(trainset.shape, validset.shape)\n",
    "testset, testlabels = load_test_data(test_data,**args)\n",
    "print(testset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization function (simulating int8 quantization)\n",
    "def quantize(x, num_bits=8):\n",
    "    scale = x.max() / (2 ** (num_bits-1) - 1)  # Scale factor for quantization\n",
    "    x_quantized = torch.round(x / scale)  # Quantize by scaling and rounding\n",
    "    x_quantized = torch.clamp(x_quantized, -2 ** (num_bits-1), 2 ** (num_bits-1) - 1)  # Clip to valid range\n",
    "    return x_quantized, scale\n",
    "\n",
    "# Dequantization function\n",
    "def dequantize(x_quantized, scale):\n",
    "    return x_quantized * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSDNN_RNN(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_size, num_layers, output_size):\n",
    "        super(FSDNN_RNN, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size=input_channels, \n",
    "                          hidden_size=hidden_size, \n",
    "                          num_layers=num_layers, \n",
    "                          batch_first=True)  # (batch, seq, features)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)  # RNN output\n",
    "        out = self.fc(out[:, -1, :])  # Take last time step for classification\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSDNN_RNN_POW2WEIGHTS_Q(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_size, num_layers, output_size):\n",
    "        super(FSDNN_RNN_POW2WEIGHTS_Q, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size=input_channels, \n",
    "                          hidden_size=hidden_size, \n",
    "                          num_layers=num_layers, \n",
    "                          batch_first=True)  # (batch, seq, features)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.bits = 8\n",
    "        self.enable_q = False\n",
    "\n",
    "    # Quantization function\n",
    "    def quantize(self, x, num_bits=8):\n",
    "        scale = x.max() / (2 ** (num_bits-1) - 1)  # Scale factor for quantization\n",
    "        x_quantized = torch.round(x / scale)  # Quantize by scaling and rounding\n",
    "        x_quantized = torch.clamp(x_quantized, -2 ** (num_bits-1), 2 ** (num_bits-1) - 1)  # Clip to valid range\n",
    "        return x_quantized, scale\n",
    "\n",
    "    # Dequantization function\n",
    "    def dequantize(self, x_quantized, scale):\n",
    "        return x_quantized * scale\n",
    "    \n",
    "    def q_sym_noscale(self, x, num_bits=8, num_frac=6):\n",
    "        s = 2 ** (num_bits - 1)\n",
    "        q = torch.round(x * s)\n",
    "        q = torch.clamp(q, -s, s - 1)\n",
    "        q = q / (2 ** num_frac)\n",
    "        return q\n",
    "    \n",
    "    def q_pow2_w(self, win):\n",
    "        sgn = torch.sign(win)\n",
    "        w_q = torch.pow(2, torch.round(torch.log2(torch.abs(win))))\n",
    "        #import pdb; pdb.set_trace()\n",
    "        return sgn * w_q  \n",
    "\n",
    "    # quantized (modified pytorch doc implementation -> fixed layered input)\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.rnn.batch_first:\n",
    "            x = x.transpose(0, 1)\n",
    "        seq_len, batch_size, _ = x.size()\n",
    "\n",
    "        h_t_minus_1 = torch.zeros(self.rnn.num_layers, batch_size, self.rnn.hidden_size)\n",
    "        h_t = torch.zeros_like(h_t_minus_1)\n",
    "\n",
    "        output = []\n",
    "\n",
    "        if self.enable_q:\n",
    "            x_quantized, scale = self.quantize(x.clone(), self.bits)\n",
    "            x = self.dequantize(x_quantized, scale)\n",
    "            #import pdb; pdb.set_trace()\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            h_t_new = []\n",
    "            for layer in range(self.rnn.num_layers):\n",
    "                if self.enable_q:\n",
    "                    weight_ih = self.q_pow2_w(getattr(self.rnn, f'weight_ih_l{layer}'))\n",
    "                    bias_ih = self.q_pow2_w(getattr(self.rnn, f'bias_ih_l{layer}'))\n",
    "                    weight_hh = self.q_pow2_w(getattr(self.rnn, f'weight_hh_l{layer}'))\n",
    "                    bias_hh = self.q_pow2_w(getattr(self.rnn, f'bias_hh_l{layer}'))\n",
    "                else:\n",
    "                    weight_ih = getattr(self.rnn, f'weight_ih_l{layer}')\n",
    "                    bias_ih = getattr(self.rnn, f'bias_ih_l{layer}')\n",
    "                    weight_hh = getattr(self.rnn, f'weight_hh_l{layer}')\n",
    "                    bias_hh = getattr(self.rnn, f'bias_hh_l{layer}')\n",
    "\n",
    "                xin = x[t] if layer == 0 else h_t_new[layer-1]\n",
    "\n",
    "                h_layer = torch.tanh(\n",
    "                    xin @ weight_ih.T\n",
    "                    + bias_ih\n",
    "                    + h_t_minus_1[layer] @ weight_hh.T\n",
    "                    + bias_hh\n",
    "                )\n",
    "\n",
    "                if self.enable_q:\n",
    "                    h_layer = self.q_sym_noscale(h_layer.clone(), self.bits, self.bits-2)\n",
    "                # h_layer_quantized, scale = self.quantize(h_layer.clone(), self.bits)\n",
    "                # h_layer_q = self.dequantize(h_layer_quantized, scale)\n",
    "                #import pdb; pdb.set_trace()\n",
    "\n",
    "                h_t_new.append(h_layer)\n",
    "\n",
    "            h_t = torch.stack(h_t_new)\n",
    "            output.append(h_t[-1])\n",
    "\n",
    "            h_t_minus_1 = h_t.detach()\n",
    "\n",
    "        output = torch.stack(output)\n",
    "        if self.rnn.batch_first:\n",
    "            output = output.transpose(0, 1)\n",
    "\n",
    "        if self.enable_q:\n",
    "            self.fc.weight.data = self.q_pow2_w(self.fc.weight.data)\n",
    "            self.fc.bias.data = self.q_pow2_w(self.fc.bias.data)\n",
    "\n",
    "        out = self.fc(output[:, -1, :])\n",
    "\n",
    "        if self.enable_q:\n",
    "            out_quantized, scale = self.quantize(out.clone(), self.bits)\n",
    "            out = self.dequantize(out_quantized, scale)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pretrained model\n",
    "model_pre = FSDNN_RNN(args['ysize'], args['rnn_hidden'], args['rnn_layers'], args['rnn_outputs'])\n",
    "model_pre.load_state_dict(torch.load('chkpt_t1.pt', weights_only=True)) #load pretrained \n",
    "\n",
    "# \n",
    "model = FSDNN_RNN_POW2WEIGHTS_Q(args['ysize'], args['rnn_hidden'], args['rnn_layers'], args['rnn_outputs'])\n",
    "pretrained_weights = model_pre.state_dict()\n",
    "new_model_dict = model.state_dict()\n",
    "pretrained_weights = {k: v for k, v in pretrained_weights.items() if k in new_model_dict}\n",
    "new_model_dict.update(pretrained_weights)\n",
    "model.load_state_dict(new_model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model for an epoch\n",
    "def train_model(trainset,trainlabels,model,optimizer,criterion,**kwargs):\n",
    "    trainlen = trainset.shape[0]\n",
    "    nbatches = math.ceil(trainlen/kwargs['batch_size'])\n",
    "    if trainlen % kwargs['batch_size'] == 1:\n",
    "        nbatches -= 1\n",
    "    total_loss = 0\n",
    "    total_backs = 0\n",
    "    with tqdm(total=nbatches,disable=(kwargs['verbose']<2)) as pbar:\n",
    "        model = model.train()\n",
    "        for b in range(nbatches):\n",
    "\n",
    "            #Obtain batch\n",
    "            X = trainset[b*kwargs['batch_size']:min(trainlen,(b+1)*kwargs['batch_size'])].clone().float()\n",
    "            X = X.to(kwargs['device'])\n",
    "            Y = trainlabels[b*kwargs['batch_size']:min(trainlen,(b+1)*kwargs['batch_size'])].clone().long().to(kwargs['device'])\n",
    "            #import pdb; pdb.set_trace()\n",
    "\n",
    "            #Propagate\n",
    "            posteriors = model(X)\n",
    "\n",
    "            #Backpropagate\n",
    "            loss = criterion(posteriors,Y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Track loss\n",
    "            if total_backs == 100:\n",
    "                total_loss = total_loss*0.99+loss.detach().cpu().numpy()\n",
    "            else:\n",
    "                total_loss += loss.detach().cpu().numpy()\n",
    "                total_backs += 1\n",
    "            pbar.set_description(f'Training epoch. Loss {total_loss/(total_backs+1):.2f}')\n",
    "            pbar.update()\n",
    "    return total_loss/(total_backs+1)\n",
    "\n",
    "#Validate last epoch's model\n",
    "def validate_model(validset,validlabels,model,**kwargs):\n",
    "    validlen = validset.shape[0]\n",
    "    acc = 0\n",
    "    total = 0\n",
    "    nbatches = math.ceil(validlen/kwargs['batch_size'])\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=nbatches,disable=(kwargs['verbose']<2)) as pbar:\n",
    "            model = model.eval()\n",
    "            for b in range(nbatches):\n",
    "                #Obtain batch\n",
    "                X = validset[b*kwargs['batch_size']:min(validlen,(b+1)*kwargs['batch_size'])].clone().float().to(kwargs['device'])\n",
    "                Y = validlabels[b*kwargs['batch_size']:min(validlen,(b+1)*kwargs['batch_size'])].clone().long().to(kwargs['device'])\n",
    "                #Propagate\n",
    "                posteriors = model(X)\n",
    "                #Accumulate accuracy\n",
    "                estimated = torch.argmax(posteriors,dim=1)\n",
    "                acc += sum((estimated.cpu().numpy() == Y.cpu().numpy()))\n",
    "                total+=Y.shape[0]\n",
    "                pbar.set_description(f'Evaluating epoch. Accuracy {100*acc/total:.2f}%')\n",
    "                pbar.update()\n",
    "    return 100*acc/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.0\n",
      "93.0\n",
      "81.33333333333333\n"
     ]
    }
   ],
   "source": [
    "# validate equivalence\n",
    "acc = validate_model(testset,testlabels,model_pre,**args)\n",
    "print(acc)\n",
    "acc = validate_model(testset,testlabels,model,**args)\n",
    "print(acc)\n",
    "model.enable_q = True\n",
    "acc = validate_model(testset,testlabels,model,**args)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 2 of 1000. Training loss: 2.17, Validation accuracy: 85.19%\n",
      "Epoch 3 of 1000. Training loss: 2.17, Validation accuracy: 85.19%\n",
      "Epoch 4 of 1000. Training loss: 2.17, Validation accuracy: 85.19%\n",
      "Epoch 5 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 6 of 1000. Training loss: 2.17, Validation accuracy: 85.19%\n",
      "Epoch 7 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 8 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 9 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 10 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 11 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 12 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 13 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 14 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 15 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 16 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 17 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 18 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 19 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 20 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 21 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 22 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 23 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 24 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 25 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 26 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 27 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 28 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 29 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 30 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 31 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 32 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 33 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 34 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 35 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 36 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 37 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 38 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 39 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 40 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 41 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 42 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 43 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 44 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 45 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 46 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 47 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 48 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 49 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 50 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 51 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 52 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 53 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 54 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 55 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 56 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 57 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 58 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 59 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 60 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 61 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 62 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 63 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 64 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 65 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 66 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 67 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 68 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 69 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 70 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 71 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 72 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 73 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 74 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 75 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 76 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 77 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 78 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 79 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 80 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 81 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 82 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 83 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 84 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 85 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 86 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 87 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 88 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 89 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 90 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 91 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 92 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 93 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 94 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 95 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 96 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 97 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 98 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 99 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 100 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 101 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 102 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 103 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 104 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 105 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 106 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 107 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 108 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 109 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 110 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 111 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n",
      "Epoch 112 of 1000. Training loss: 2.18, Validation accuracy: 85.19%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m## Training Loop\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#Do backpropgation and validation epochs\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrainlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     13\u001b[0m     acc \u001b[38;5;241m=\u001b[39m validate_model(validset,validlabels,model,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)\n",
      "Cell \u001b[0;32mIn[16], line 20\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(trainset, trainlabels, model, optimizer, criterion, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m Y \u001b[38;5;241m=\u001b[39m trainlabels[b\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]:\u001b[38;5;28mmin\u001b[39m(trainlen,(b\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m])]\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#import pdb; pdb.set_trace()\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#Propagate\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m posteriors \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#Backpropagate\u001b[39;00m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(posteriors,Y)\n",
      "File \u001b[0;32m~/9_sandbox/fsdd_constrained/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/9_sandbox/fsdd_constrained/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[14], line 60\u001b[0m, in \u001b[0;36mFSDNN_RNN_POW2WEIGHTS_Q.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m     weight_ih \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_pow2_w(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_ih_l\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     59\u001b[0m     bias_ih \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_pow2_w(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias_ih_l\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 60\u001b[0m     weight_hh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_pow2_w\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_hh_l\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlayer\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     bias_hh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_pow2_w(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias_hh_l\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[14], line 33\u001b[0m, in \u001b[0;36mFSDNN_RNN_POW2WEIGHTS_Q.q_pow2_w\u001b[0;34m(self, win)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mq_pow2_w\u001b[39m(\u001b[38;5;28mself\u001b[39m, win):\n\u001b[1;32m     32\u001b[0m     sgn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msign(win)\n\u001b[0;32m---> 33\u001b[0m     w_q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m, torch\u001b[38;5;241m.\u001b[39mround(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwin\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m#import pdb; pdb.set_trace()\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sgn \u001b[38;5;241m*\u001b[39m w_q\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# QAT\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "## Training Setup\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=args['learning_rate'])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "## Training Loop\n",
    "for ep in range(1,args['epochs']+1):\n",
    "    #Do backpropgation and validation epochs\n",
    "    loss = train_model(trainset,trainlabels,model,optimizer,criterion,**args)\n",
    "    scheduler.step()\n",
    "    acc = validate_model(validset,validlabels,model,**args)\n",
    "    print('Epoch {0:d} of {1:d}. Training loss: {2:.2f}, Validation accuracy: {3:.2f}%'.format(ep,args['epochs'],loss,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'chkpt_t3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81.33333333333333"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = validate_model(testset,testlabels,model,**args)\n",
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
