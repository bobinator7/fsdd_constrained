{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task B: Train RNN on FSDD \n",
    "# - full precision 32 float\n",
    "# - close to comparable designs (current deviation -6%, fixable by scaling network and tuning hyperparameters)\n",
    "# - RNN layer sizing (64x64x2+64x2)x4bytes = ~33kB < 36 kB from Task B constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch.quantization as quantization\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sigproc import gen_logmel, feat2img\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mainly adapted from https://github.com/saztorralba/CNNWordReco due to following\n",
    "# - deeplake / hub version broken -> replaced with original wavs (cloned orig repo: https://github.com/Jakobovski/free-spoken-digit-dataset)\n",
    "# - logmel suitable for detection of spoken speech -> normalized, resampled, high-pass filtered, time axis scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'train_val_percentage': 0.1,\n",
    "    'xsize': 20,\n",
    "    'ysize': 20,\n",
    "    'rnn_layers': 3,\n",
    "    'rnn_hidden': 64,\n",
    "    'rnn_outputs': 10,\n",
    "    'epochs': 1000,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "    'device': 'cpu',\n",
    "    'verbose': 1,\n",
    "    'augment': False,\n",
    "    'vocab': OrderedDict({'ZERO': 0, 'ONE': 1, 'TWO': 2, 'THREE': 3, 'FOUR': 4, 'FIVE': 5, 'SIX': 6, 'SEVEN': 7, 'EIGHT': 8, 'NINE': 9})\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## labels and paths in pd frame\n",
    "wavfiles = glob.glob('./free-spoken-digit-dataset/recordings/*.wav')\n",
    "speakers = [file.split('/')[-1].split('_')[1] for file in wavfiles]\n",
    "words = [list(args['vocab'].keys())[int(file.split('/')[-1].split('_')[0])] for file in wavfiles]\n",
    "rec_number = [int(file.split('/')[-1].split('_')[2].split('.')[0]) for file in wavfiles]\n",
    "data = pd.DataFrame({'wavfile':wavfiles,'speaker':speakers,'word':words,'rec_number':rec_number})\n",
    "\n",
    "## train/test split according to https://github.com/Jakobovski/free-spoken-digit-dataset\n",
    "train_data = data.loc[data['rec_number']>=5].reset_index(drop=True)\n",
    "test_data = data.loc[data['rec_number']<5].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log mels for audio; time scaled by PIL.Image to xsize, 40 nmels\n",
    "def load_data(data,cv=False,**kwargs):\n",
    "    n_samples = len(data)\n",
    "    dataset = torch.zeros((n_samples,kwargs['ysize'],kwargs['xsize']),dtype=torch.uint8)\n",
    "    labels = torch.zeros((n_samples),dtype=torch.uint8)\n",
    "    for i in tqdm(range(n_samples),disable=(kwargs['verbose']<2)):\n",
    "        path = data['wavfile'][i]\n",
    "        dataset[i,:,:] = torch.from_numpy(feat2img(gen_logmel(path,(kwargs['n_mels'] if 'n_mels' in kwargs else 40),(kwargs['sampling'] if 'sampling' in kwargs else 8000),True),kwargs['ysize'],kwargs['xsize']))\n",
    "        labels[i] = kwargs['vocab'][data['word'][i]]\n",
    "\n",
    "    if cv == False:\n",
    "        return dataset, labels\n",
    "\n",
    "    #Do random train/validation split\n",
    "    idx = [i for i in range(n_samples)]\n",
    "    random.shuffle(idx)\n",
    "    trainset = dataset[idx[0:int(n_samples*(1-kwargs['train_val_percentage']))]]\n",
    "    trainlabels = labels[idx[0:int(n_samples*(1-kwargs['train_val_percentage']))]]\n",
    "    validset = dataset[idx[int(n_samples*(1-kwargs['train_val_percentage'])):]]\n",
    "    validlabels = labels[idx[int(n_samples*(1-kwargs['train_val_percentage'])):]]\n",
    "    return trainset, validset, trainlabels, validlabels\n",
    "\n",
    "def load_test_data(data,**kwargs):\n",
    "    n_samples = len(data)\n",
    "    dataset = torch.zeros((n_samples,kwargs['ysize'],kwargs['xsize']),dtype=torch.uint8)\n",
    "    labels = torch.zeros((n_samples),dtype=torch.uint8)\n",
    "    for i in tqdm(range(n_samples),disable=(kwargs['verbose']<2)):\n",
    "        path = data['wavfile'][i]\n",
    "        dataset[i,:,:] = torch.from_numpy(feat2img(gen_logmel(path,(kwargs['n_mels'] if 'n_mels' in kwargs else 40),(kwargs['sampling'] if 'sampling' in kwargs else 8000),True),kwargs['ysize'],kwargs['xsize']))\n",
    "        labels[i] = kwargs['vocab'][data['word'][i]]\n",
    "\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2430, 20, 20]) torch.Size([270, 20, 20])\n",
      "torch.Size([300, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "trainset, validset, trainlabels, validlabels = load_data(train_data,True,**args)\n",
    "print(trainset.shape, validset.shape)\n",
    "testset, testlabels = load_test_data(test_data,**args)\n",
    "print(testset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization function (simulating int8 quantization)\n",
    "def quantize(x, num_bits=8):\n",
    "    scale = x.max() / (2 ** (num_bits-1) - 1)  # Scale factor for quantization\n",
    "    x_quantized = torch.round(x / scale)  # Quantize by scaling and rounding\n",
    "    x_quantized = torch.clamp(x_quantized, -2 ** (num_bits-1), 2 ** (num_bits-1) - 1)  # Clip to valid range\n",
    "    return x_quantized, scale\n",
    "\n",
    "# Dequantization function\n",
    "def dequantize(x_quantized, scale):\n",
    "    return x_quantized * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSDNN_RNN(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_size, num_layers, output_size):\n",
    "        super(FSDNN_RNN, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size=input_channels, \n",
    "                          hidden_size=hidden_size, \n",
    "                          num_layers=num_layers, \n",
    "                          batch_first=True)  # (batch, seq, features)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)  # RNN output\n",
    "        out = self.fc(out[:, -1, :])  # Take last time step for classification\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSDNN_RNN_Q(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_size, num_layers, output_size):\n",
    "        super(FSDNN_RNN_Q, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size=input_channels, \n",
    "                          hidden_size=hidden_size, \n",
    "                          num_layers=num_layers, \n",
    "                          batch_first=True)  # (batch, seq, features)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.bits = 8\n",
    "        self.enable_q = False\n",
    "\n",
    "    # Quantization function\n",
    "    def quantize(self, x, num_bits=8):\n",
    "        scale = x.max() / (2 ** (num_bits-1) - 1)  # Scale factor for quantization\n",
    "        x_quantized = torch.round(x / scale)  # Quantize by scaling and rounding\n",
    "        x_quantized = torch.clamp(x_quantized, -2 ** (num_bits-1), 2 ** (num_bits-1) - 1)  # Clip to valid range\n",
    "        return x_quantized, scale\n",
    "\n",
    "    # Dequantization function\n",
    "    def dequantize(self, x_quantized, scale):\n",
    "        return x_quantized * scale\n",
    "    \n",
    "    def q_sym_noscale(self, x, num_bits=8, num_frac=6):\n",
    "        s = 2 ** (num_bits - 1)\n",
    "        q = torch.round(x * s)\n",
    "        q = torch.clamp(q, -s, s - 1)\n",
    "        q = q / (2 ** num_frac)\n",
    "        return q\n",
    "\n",
    "    # quantized (modified pytorch doc implementation -> fixed layered input)\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.rnn.batch_first:\n",
    "            x = x.transpose(0, 1)\n",
    "        seq_len, batch_size, _ = x.size()\n",
    "\n",
    "        h_t_minus_1 = torch.zeros(self.rnn.num_layers, batch_size, self.rnn.hidden_size)\n",
    "        h_t = torch.zeros_like(h_t_minus_1)\n",
    "\n",
    "        output = []\n",
    "\n",
    "        if self.enable_q:\n",
    "            x_quantized, scale = self.quantize(x.clone(), self.bits)\n",
    "            x = self.dequantize(x_quantized, scale)\n",
    "            #import pdb; pdb.set_trace()\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            h_t_new = []\n",
    "            for layer in range(self.rnn.num_layers):\n",
    "                if self.enable_q:\n",
    "                    weight_ih = self.q_sym_noscale(getattr(self.rnn, f'weight_ih_l{layer}'), self.bits, self.bits-2)\n",
    "                    bias_ih = self.q_sym_noscale(getattr(self.rnn, f'bias_ih_l{layer}'), self.bits, self.bits-2)\n",
    "                    weight_hh = self.q_sym_noscale(getattr(self.rnn, f'weight_hh_l{layer}'), self.bits, self.bits-2)\n",
    "                    bias_hh = self.q_sym_noscale(getattr(self.rnn, f'bias_hh_l{layer}'), self.bits, self.bits-2)\n",
    "                else:\n",
    "                    weight_ih = getattr(self.rnn, f'weight_ih_l{layer}')\n",
    "                    bias_ih = getattr(self.rnn, f'bias_ih_l{layer}')\n",
    "                    weight_hh = getattr(self.rnn, f'weight_hh_l{layer}')\n",
    "                    bias_hh = getattr(self.rnn, f'bias_hh_l{layer}')\n",
    "\n",
    "                xin = x[t] if layer == 0 else h_t_new[layer-1]\n",
    "\n",
    "                h_layer = torch.tanh(\n",
    "                    xin @ weight_ih.T\n",
    "                    + bias_ih\n",
    "                    + h_t_minus_1[layer] @ weight_hh.T\n",
    "                    + bias_hh\n",
    "                )\n",
    "\n",
    "                if self.enable_q:\n",
    "                    h_layer = self.q_sym_noscale(h_layer.clone(), self.bits, self.bits-2)\n",
    "                # h_layer_quantized, scale = self.quantize(h_layer.clone(), self.bits)\n",
    "                # h_layer_q = self.dequantize(h_layer_quantized, scale)\n",
    "                #import pdb; pdb.set_trace()\n",
    "\n",
    "                h_t_new.append(h_layer)\n",
    "\n",
    "            h_t = torch.stack(h_t_new)\n",
    "            output.append(h_t[-1])\n",
    "\n",
    "            h_t_minus_1 = h_t.detach()\n",
    "\n",
    "        output = torch.stack(output)\n",
    "        if self.rnn.batch_first:\n",
    "            output = output.transpose(0, 1)\n",
    "\n",
    "        out = self.fc(output[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pretrained model\n",
    "model_pre = FSDNN_RNN(args['ysize'], args['rnn_hidden'], args['rnn_layers'], args['rnn_outputs'])\n",
    "model_pre.load_state_dict(torch.load('chkpt_t1.pt', weights_only=True)) #load pretrained \n",
    "\n",
    "# \n",
    "model = FSDNN_RNN_Q(args['ysize'], args['rnn_hidden'], args['rnn_layers'], args['rnn_outputs'])\n",
    "pretrained_weights = model_pre.state_dict()\n",
    "new_model_dict = model.state_dict()\n",
    "pretrained_weights = {k: v for k, v in pretrained_weights.items() if k in new_model_dict}\n",
    "new_model_dict.update(pretrained_weights)\n",
    "model.load_state_dict(new_model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 weight_ih diff: 0.0\n",
      "Layer 0 weight_hh diff: 0.0\n",
      "Layer 1 weight_ih diff: 0.0\n",
      "Layer 1 weight_hh diff: 0.0\n",
      "Layer 2 weight_ih diff: 0.0\n",
      "Layer 2 weight_hh diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "for layer in range(model.rnn.num_layers):\n",
    "    print(f\"Layer {layer} weight_ih diff:\", torch.norm(\n",
    "        getattr(model.rnn, f'weight_ih_l{layer}') - getattr(model_pre.rnn, f'weight_ih_l{layer}')\n",
    "    ).item())\n",
    "    print(f\"Layer {layer} weight_hh diff:\", torch.norm(\n",
    "        getattr(model.rnn, f'weight_hh_l{layer}') - getattr(model_pre.rnn, f'weight_hh_l{layer}')\n",
    "    ).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model for an epoch\n",
    "def train_model(trainset,trainlabels,model,optimizer,criterion,**kwargs):\n",
    "    trainlen = trainset.shape[0]\n",
    "    nbatches = math.ceil(trainlen/kwargs['batch_size'])\n",
    "    if trainlen % kwargs['batch_size'] == 1:\n",
    "        nbatches -= 1\n",
    "    total_loss = 0\n",
    "    total_backs = 0\n",
    "    with tqdm(total=nbatches,disable=(kwargs['verbose']<2)) as pbar:\n",
    "        model = model.train()\n",
    "        for b in range(nbatches):\n",
    "\n",
    "            #Obtain batch\n",
    "            X = trainset[b*kwargs['batch_size']:min(trainlen,(b+1)*kwargs['batch_size'])].clone().float()\n",
    "            X = X.to(kwargs['device'])\n",
    "            Y = trainlabels[b*kwargs['batch_size']:min(trainlen,(b+1)*kwargs['batch_size'])].clone().long().to(kwargs['device'])\n",
    "            #import pdb; pdb.set_trace()\n",
    "\n",
    "            #Propagate\n",
    "            posteriors = model(X)\n",
    "\n",
    "            #Backpropagate\n",
    "            loss = criterion(posteriors,Y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Track loss\n",
    "            if total_backs == 100:\n",
    "                total_loss = total_loss*0.99+loss.detach().cpu().numpy()\n",
    "            else:\n",
    "                total_loss += loss.detach().cpu().numpy()\n",
    "                total_backs += 1\n",
    "            pbar.set_description(f'Training epoch. Loss {total_loss/(total_backs+1):.2f}')\n",
    "            pbar.update()\n",
    "    return total_loss/(total_backs+1)\n",
    "\n",
    "#Validate last epoch's model\n",
    "def validate_model(validset,validlabels,model,**kwargs):\n",
    "    validlen = validset.shape[0]\n",
    "    acc = 0\n",
    "    total = 0\n",
    "    nbatches = math.ceil(validlen/kwargs['batch_size'])\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=nbatches,disable=(kwargs['verbose']<2)) as pbar:\n",
    "            model = model.eval()\n",
    "            for b in range(nbatches):\n",
    "                #Obtain batch\n",
    "                X = validset[b*kwargs['batch_size']:min(validlen,(b+1)*kwargs['batch_size'])].clone().float().to(kwargs['device'])\n",
    "                Y = validlabels[b*kwargs['batch_size']:min(validlen,(b+1)*kwargs['batch_size'])].clone().long().to(kwargs['device'])\n",
    "                #Propagate\n",
    "                posteriors = model(X)\n",
    "                #Accumulate accuracy\n",
    "                estimated = torch.argmax(posteriors,dim=1)\n",
    "                acc += sum((estimated.cpu().numpy() == Y.cpu().numpy()))\n",
    "                total+=Y.shape[0]\n",
    "                pbar.set_description(f'Evaluating epoch. Accuracy {100*acc/total:.2f}%')\n",
    "                pbar.update()\n",
    "    return 100*acc/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.0\n",
      "93.0\n",
      "81.33333333333333\n"
     ]
    }
   ],
   "source": [
    "# validate equivalence\n",
    "acc = validate_model(testset,testlabels,model_pre,**args)\n",
    "print(acc)\n",
    "acc = validate_model(testset,testlabels,model,**args)\n",
    "print(acc)\n",
    "model.enable_q = True\n",
    "acc = validate_model(testset,testlabels,model,**args)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1000. Training loss: 2.37, Validation accuracy: 84.44%\n",
      "Epoch 2 of 1000. Training loss: 2.20, Validation accuracy: 85.19%\n",
      "Epoch 3 of 1000. Training loss: 2.10, Validation accuracy: 85.19%\n",
      "Epoch 4 of 1000. Training loss: 2.02, Validation accuracy: 85.19%\n",
      "Epoch 5 of 1000. Training loss: 1.96, Validation accuracy: 85.19%\n",
      "Epoch 6 of 1000. Training loss: 1.90, Validation accuracy: 85.19%\n",
      "Epoch 7 of 1000. Training loss: 1.84, Validation accuracy: 85.19%\n",
      "Epoch 8 of 1000. Training loss: 1.78, Validation accuracy: 85.19%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m## Training Loop\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#Do backpropgation and validation epochs\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrainlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     13\u001b[0m     acc \u001b[38;5;241m=\u001b[39m validate_model(validset,validlabels,model,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)\n",
      "Cell \u001b[0;32mIn[13], line 20\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(trainset, trainlabels, model, optimizer, criterion, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m Y \u001b[38;5;241m=\u001b[39m trainlabels[b\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]:\u001b[38;5;28mmin\u001b[39m(trainlen,(b\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m])]\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#import pdb; pdb.set_trace()\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#Propagate\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m posteriors \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#Backpropagate\u001b[39;00m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(posteriors,Y)\n",
      "File \u001b[0;32m~/9_sandbox/fsdd_constrained/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/9_sandbox/fsdd_constrained/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[10], line 53\u001b[0m, in \u001b[0;36mFSDNN_RNN_Q.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_q:\n\u001b[1;32m     52\u001b[0m     weight_ih \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_sym_noscale(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_ih_l\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbits, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbits\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m     bias_ih \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_sym_noscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbias_ih_l\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlayer\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbits\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     weight_hh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_sym_noscale(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_hh_l\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbits, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbits\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     55\u001b[0m     bias_hh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_sym_noscale(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias_hh_l\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbits, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbits\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 26\u001b[0m, in \u001b[0;36mFSDNN_RNN_Q.q_sym_noscale\u001b[0;34m(self, x, num_bits, num_frac)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mq_sym_noscale\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, num_bits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, num_frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m):\n\u001b[1;32m     25\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (num_bits \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mround\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(q, \u001b[38;5;241m-\u001b[39ms, s \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m     q \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m num_frac)\n",
      "File \u001b[0;32m~/9_sandbox/fsdd_constrained/.venv/lib/python3.10/site-packages/torch/fx/traceback.py:175\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [current_meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstack_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/traceback.py:39\u001b[0m, in \u001b[0;36mformat_list\u001b[0;34m(extracted_list)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mformat_list\u001b[39m(extracted_list):\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format a list of tuples or FrameSummary objects for printing.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    Given a list of tuples or FrameSummary objects as returned by\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    whose source text line is not None.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextracted_list\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/traceback.py:440\u001b[0m, in \u001b[0;36mStackSummary.format\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    439\u001b[0m row \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 440\u001b[0m row\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m  File \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, line \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m, in \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlineno\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame\u001b[38;5;241m.\u001b[39mline:\n\u001b[1;32m    443\u001b[0m     row\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m    \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(frame\u001b[38;5;241m.\u001b[39mline\u001b[38;5;241m.\u001b[39mstrip()))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# QAT\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "## Training Setup\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=args['learning_rate'])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "## Training Loop\n",
    "for ep in range(1,args['epochs']+1):\n",
    "    #Do backpropgation and validation epochs\n",
    "    loss = train_model(trainset,trainlabels,model,optimizer,criterion,**args)\n",
    "    scheduler.step()\n",
    "    acc = validate_model(validset,validlabels,model,**args)\n",
    "    print('Epoch {0:d} of {1:d}. Training loss: {2:.2f}, Validation accuracy: {3:.2f}%'.format(ep,args['epochs'],loss,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'chkpt_t2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = validate_model(testset,testlabels,model,**args)\n",
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
