{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task B: Train RNN on FSDD (quantized)\n",
    "# - full precision 8 bit precision (mixed-precision)\n",
    "# - scaling factor unrealistic for fully fixed point implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "\n",
    "from utils import get_rec_paths, load_data, train_model, validate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2430, 20, 20]) torch.Size([270, 20, 20]) torch.Size([300, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "# Load from YAML file\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    args = yaml.safe_load(f)\n",
    "\n",
    "# labels and paths in pd frame\n",
    "data = get_rec_paths('./free-spoken-digit-dataset/recordings')\n",
    "\n",
    "# load train, val, test data\n",
    "trainset, validset, trainlabels, validlabels, testset, testlabels = load_data(data,True,**args)\n",
    "print(trainset.shape, validset.shape, testset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSDNN_RNN(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_size, num_layers, output_size):\n",
    "        super(FSDNN_RNN, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size=input_channels, \n",
    "                          hidden_size=hidden_size, \n",
    "                          num_layers=num_layers, \n",
    "                          batch_first=True)  # (batch, seq, features)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)  # RNN output\n",
    "        out = self.fc(out[:, -1, :])  # Take last time step for classification\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSDNN_RNN_Q(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_size, num_layers, output_size):\n",
    "        super(FSDNN_RNN_Q, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size=input_channels, \n",
    "                          hidden_size=hidden_size, \n",
    "                          num_layers=num_layers, \n",
    "                          batch_first=True)  # (batch, seq, features)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.bits = 8\n",
    "        self.enable_q = False\n",
    "\n",
    "    # Quantization function\n",
    "    def quantize(self, x, num_bits=8):\n",
    "        scale = x.max() / (2 ** (num_bits-1) - 1)  # Scale factor for quantization\n",
    "        x_quantized = torch.round(x / scale)  # Quantize by scaling and rounding\n",
    "        x_quantized = torch.clamp(x_quantized, -2 ** (num_bits-1), 2 ** (num_bits-1) - 1)  # Clip to valid range\n",
    "        return x_quantized, scale\n",
    "\n",
    "    # Dequantization function\n",
    "    def dequantize(self, x_quantized, scale):\n",
    "        return x_quantized * scale\n",
    "    \n",
    "    def q_sym_noscale(self, x, num_bits=8, num_frac=6):\n",
    "        s = 2 ** (num_bits - 1)\n",
    "        q = torch.round(x * s)\n",
    "        q = torch.clamp(q, -s, s - 1)\n",
    "        q = q / (2 ** num_frac)\n",
    "        return q\n",
    "\n",
    "    # quantized (modified pytorch doc implementation -> fixed layered input)\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.rnn.batch_first:\n",
    "            x = x.transpose(0, 1)\n",
    "        seq_len, batch_size, _ = x.size()\n",
    "\n",
    "        h_t_minus_1 = torch.zeros(self.rnn.num_layers, batch_size, self.rnn.hidden_size)\n",
    "        h_t = torch.zeros_like(h_t_minus_1)\n",
    "\n",
    "        output = []\n",
    "\n",
    "        if self.enable_q:\n",
    "            x_quantized, scale = self.quantize(x.clone(), self.bits)\n",
    "            x = self.dequantize(x_quantized, scale)\n",
    "            #import pdb; pdb.set_trace()\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            h_t_new = []\n",
    "            for layer in range(self.rnn.num_layers):\n",
    "                if self.enable_q:\n",
    "                    weight_ih = self.q_sym_noscale(getattr(self.rnn, f'weight_ih_l{layer}'), self.bits, self.bits-2)\n",
    "                    bias_ih = self.q_sym_noscale(getattr(self.rnn, f'bias_ih_l{layer}'), self.bits, self.bits-2)\n",
    "                    weight_hh = self.q_sym_noscale(getattr(self.rnn, f'weight_hh_l{layer}'), self.bits, self.bits-2)\n",
    "                    bias_hh = self.q_sym_noscale(getattr(self.rnn, f'bias_hh_l{layer}'), self.bits, self.bits-2)\n",
    "\n",
    "                    getattr(self.rnn, f'weight_ih_l{layer}').data = weight_ih\n",
    "                    getattr(self.rnn, f'bias_ih_l{layer}').data = bias_ih\n",
    "                    getattr(self.rnn, f'weight_hh_l{layer}').data = weight_hh\n",
    "                    getattr(self.rnn, f'bias_hh_l{layer}').data = bias_hh\n",
    "                else:\n",
    "                    weight_ih = getattr(self.rnn, f'weight_ih_l{layer}')\n",
    "                    bias_ih = getattr(self.rnn, f'bias_ih_l{layer}')\n",
    "                    weight_hh = getattr(self.rnn, f'weight_hh_l{layer}')\n",
    "                    bias_hh = getattr(self.rnn, f'bias_hh_l{layer}')\n",
    "\n",
    "                xin = x[t] if layer == 0 else h_t_new[layer-1]\n",
    "\n",
    "                h_layer = torch.tanh(\n",
    "                    xin @ weight_ih.T\n",
    "                    + bias_ih\n",
    "                    + h_t_minus_1[layer] @ weight_hh.T\n",
    "                    + bias_hh\n",
    "                )\n",
    "\n",
    "                if self.enable_q:\n",
    "                    h_layer = self.q_sym_noscale(h_layer.clone(), self.bits, self.bits-2)\n",
    "                    # h_layer_quantized, scale = self.quantize(h_layer.clone(), self.bits)\n",
    "                    # h_layer_q = self.dequantize(h_layer_quantized, scale)\n",
    "                #import pdb; pdb.set_trace()\n",
    "\n",
    "                h_t_new.append(h_layer)\n",
    "\n",
    "            h_t = torch.stack(h_t_new)\n",
    "            output.append(h_t[-1])\n",
    "\n",
    "            h_t_minus_1 = h_t.detach()\n",
    "\n",
    "        output = torch.stack(output)\n",
    "        if self.rnn.batch_first:\n",
    "            output = output.transpose(0, 1)\n",
    "\n",
    "        if self.enable_q:\n",
    "            self.fc.weight.data = self.q_sym_noscale(self.fc.weight.data, self.bits, self.bits - 2)\n",
    "            self.fc.bias.data = self.q_sym_noscale(self.fc.bias.data, self.bits, self.bits - 2)\n",
    "\n",
    "        out = self.fc(output[:, -1, :])\n",
    "\n",
    "        if self.enable_q:\n",
    "            out_quantized, scale = self.quantize(out.clone(), self.bits)\n",
    "            out = self.dequantize(out_quantized, scale)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pretrained model\n",
    "model_pre = FSDNN_RNN(args['ysize'], args['rnn_hidden'], args['rnn_layers'], args['rnn_outputs'])\n",
    "model_pre.load_state_dict(torch.load('chkpt_t1.pt', weights_only=True)) #load pretrained \n",
    "\n",
    "# \n",
    "model = FSDNN_RNN_Q(args['ysize'], args['rnn_hidden'], args['rnn_layers'], args['rnn_outputs'])\n",
    "pretrained_weights = model_pre.state_dict()\n",
    "new_model_dict = model.state_dict()\n",
    "pretrained_weights = {k: v for k, v in pretrained_weights.items() if k in new_model_dict}\n",
    "new_model_dict.update(pretrained_weights)\n",
    "model.load_state_dict(new_model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1432, -0.0888,  0.0867,  ..., -0.0444,  0.1106,  0.0258],\n",
       "        [ 0.1011, -0.0047,  0.0121,  ..., -0.0466, -0.1202, -0.0847],\n",
       "        [ 0.3043,  0.1816,  0.0604,  ..., -0.0505, -0.0778,  0.0912],\n",
       "        ...,\n",
       "        [ 0.1131, -0.1353,  0.0475,  ..., -0.1440, -0.0864, -0.2182],\n",
       "        [ 0.0720, -0.1547, -0.1590,  ...,  0.0074,  0.0366, -0.0103],\n",
       "        [-0.0082, -0.0992, -0.0657,  ...,  0.0088, -0.0150, -0.0058]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.rnn.weight_ih_l0.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.66666666666667\n",
      "91.66666666666667\n",
      "78.33333333333333\n"
     ]
    }
   ],
   "source": [
    "# validate equivalence\n",
    "acc = validate_model(testset,testlabels,model_pre,**args)\n",
    "print(acc)\n",
    "acc = validate_model(testset,testlabels,model,**args)\n",
    "print(acc)\n",
    "model.enable_q = True\n",
    "acc = validate_model(testset,testlabels,model,**args)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1000. Training loss: 7.29, Validation accuracy: 79.26%\n",
      "Epoch 2 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 3 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 4 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 5 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 6 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 7 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 8 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 9 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 10 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 11 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 12 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 13 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 14 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 15 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 16 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 17 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 18 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 19 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 20 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 21 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 22 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 23 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 24 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 25 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 26 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 27 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 28 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 29 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 30 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 31 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 32 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 33 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 34 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 35 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 36 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 37 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 38 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 39 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 40 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 41 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 42 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 43 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 44 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 45 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 46 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 47 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 48 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 49 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 50 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 51 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 52 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 53 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 54 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n",
      "Epoch 55 of 1000. Training loss: 7.28, Validation accuracy: 79.26%\n"
     ]
    }
   ],
   "source": [
    "# QAT\n",
    "## Training Setup\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=args['learning_rate'])\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "## Training Loop\n",
    "acc_best = 0\n",
    "for ep in range(1,args['epochs']+1):\n",
    "\n",
    "    # training\n",
    "    loss = train_model(trainset,trainlabels,model,optimizer,criterion,**args)\n",
    "    #scheduler.step()\n",
    "    acc = validate_model(validset,validlabels,model,**args)\n",
    "\n",
    "    # save best model\n",
    "    if acc > acc_best:\n",
    "        acc_best = acc\n",
    "        torch.save(model.state_dict(), 'chkpt_2.pt')    \n",
    "\n",
    "    # display progress\n",
    "    # if ep % 10 == 0:\n",
    "    #     print('Epoch {0:d} of {1:d}. Training loss: {2:.2f}, Validation accuracy: {3:.2f}%'.format(ep,args['epochs'],loss,acc))\n",
    "    print('Epoch {0:d} of {1:d}. Training loss: {2:.2f}, Validation accuracy: {3:.2f}%'.format(ep,args['epochs'],loss,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = validate_model(testset,testlabels,model,**args)\n",
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
