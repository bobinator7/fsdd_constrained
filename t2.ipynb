{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task B: Train RNN on FSDD \n",
    "# - full precision 32 float\n",
    "# - close to comparable designs (current deviation -6%, fixable by scaling network and tuning hyperparameters)\n",
    "# - RNN layer sizing (64x64x2+64x2)x4bytes = ~33kB < 36 kB from Task B constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch.quantization as quantization\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sigproc import gen_logmel, feat2img\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mainly adapted from https://github.com/saztorralba/CNNWordReco due to following\n",
    "# - deeplake / hub version broken -> replaced with original wavs (cloned orig repo: https://github.com/Jakobovski/free-spoken-digit-dataset)\n",
    "# - logmel suitable for detection of spoken speech -> normalized, resampled, high-pass filtered, time axis scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'train_val_percentage': 0.1,\n",
    "    'xsize': 20,\n",
    "    'ysize': 20,\n",
    "    'rnn_layers': 3,\n",
    "    'rnn_hidden': 64,\n",
    "    'rnn_outputs': 10,\n",
    "    'epochs': 1000,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "    'device': 'cpu',\n",
    "    'verbose': 1,\n",
    "    'augment': False,\n",
    "    'vocab': OrderedDict({'ZERO': 0, 'ONE': 1, 'TWO': 2, 'THREE': 3, 'FOUR': 4, 'FIVE': 5, 'SIX': 6, 'SEVEN': 7, 'EIGHT': 8, 'NINE': 9})\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## labels and paths in pd frame\n",
    "wavfiles = glob.glob('./free-spoken-digit-dataset/recordings/*.wav')\n",
    "speakers = [file.split('/')[-1].split('_')[1] for file in wavfiles]\n",
    "words = [list(args['vocab'].keys())[int(file.split('/')[-1].split('_')[0])] for file in wavfiles]\n",
    "rec_number = [int(file.split('/')[-1].split('_')[2].split('.')[0]) for file in wavfiles]\n",
    "data = pd.DataFrame({'wavfile':wavfiles,'speaker':speakers,'word':words,'rec_number':rec_number})\n",
    "\n",
    "## train/test split according to https://github.com/Jakobovski/free-spoken-digit-dataset\n",
    "train_data = data.loc[data['rec_number']>=5].reset_index(drop=True)\n",
    "test_data = data.loc[data['rec_number']<5].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log mels for audio; time scaled by PIL.Image to xsize, 40 nmels\n",
    "def load_data(data,cv=False,**kwargs):\n",
    "    n_samples = len(data)\n",
    "    dataset = torch.zeros((n_samples,kwargs['ysize'],kwargs['xsize']),dtype=torch.uint8)\n",
    "    labels = torch.zeros((n_samples),dtype=torch.uint8)\n",
    "    for i in tqdm(range(n_samples),disable=(kwargs['verbose']<2)):\n",
    "        path = data['wavfile'][i]\n",
    "        dataset[i,:,:] = torch.from_numpy(feat2img(gen_logmel(path,(kwargs['n_mels'] if 'n_mels' in kwargs else 40),(kwargs['sampling'] if 'sampling' in kwargs else 8000),True),kwargs['ysize'],kwargs['xsize']))\n",
    "        labels[i] = kwargs['vocab'][data['word'][i]]\n",
    "\n",
    "    if cv == False:\n",
    "        return dataset, labels\n",
    "\n",
    "    #Do random train/validation split\n",
    "    idx = [i for i in range(n_samples)]\n",
    "    random.shuffle(idx)\n",
    "    trainset = dataset[idx[0:int(n_samples*(1-kwargs['train_val_percentage']))]]\n",
    "    trainlabels = labels[idx[0:int(n_samples*(1-kwargs['train_val_percentage']))]]\n",
    "    validset = dataset[idx[int(n_samples*(1-kwargs['train_val_percentage'])):]]\n",
    "    validlabels = labels[idx[int(n_samples*(1-kwargs['train_val_percentage'])):]]\n",
    "    return trainset, validset, trainlabels, validlabels\n",
    "\n",
    "def load_test_data(data,**kwargs):\n",
    "    n_samples = len(data)\n",
    "    dataset = torch.zeros((n_samples,kwargs['ysize'],kwargs['xsize']),dtype=torch.uint8)\n",
    "    labels = torch.zeros((n_samples),dtype=torch.uint8)\n",
    "    for i in tqdm(range(n_samples),disable=(kwargs['verbose']<2)):\n",
    "        path = data['wavfile'][i]\n",
    "        dataset[i,:,:] = torch.from_numpy(feat2img(gen_logmel(path,(kwargs['n_mels'] if 'n_mels' in kwargs else 40),(kwargs['sampling'] if 'sampling' in kwargs else 8000),True),kwargs['ysize'],kwargs['xsize']))\n",
    "        labels[i] = kwargs['vocab'][data['word'][i]]\n",
    "\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2430, 20, 20]) torch.Size([270, 20, 20])\n",
      "torch.Size([300, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "trainset, validset, trainlabels, validlabels = load_data(train_data,True,**args)\n",
    "print(trainset.shape, validset.shape)\n",
    "testset, testlabels = load_test_data(test_data,**args)\n",
    "print(testset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization function (simulating int8 quantization)\n",
    "def quantize(x, num_bits=8):\n",
    "    scale = x.max() / (2 ** num_bits - 1)  # Scale factor for quantization\n",
    "    x_quantized = torch.round(x / scale)  # Quantize by scaling and rounding\n",
    "    x_quantized = torch.clamp(x_quantized, 0, 2 ** num_bits - 1)  # Clip to valid range\n",
    "    return x_quantized, scale\n",
    "\n",
    "# Dequantization function\n",
    "def dequantize(x_quantized, scale):\n",
    "    return x_quantized * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSDNN_RNN(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_size, num_layers, output_size):\n",
    "        super(FSDNN_RNN, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size=input_channels, \n",
    "                          hidden_size=hidden_size, \n",
    "                          num_layers=num_layers, \n",
    "                          batch_first=True)  # (batch, seq, features)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)  # RNN output\n",
    "        out = self.fc(out[:, -1, :])  # Take last time step for classification\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSDNN_RNN_Q(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_size, num_layers, output_size):\n",
    "        super(FSDNN_RNN_Q, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size=input_channels, \n",
    "                          hidden_size=hidden_size, \n",
    "                          num_layers=num_layers, \n",
    "                          batch_first=True)  # (batch, seq, features)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.bits = 8\n",
    "        self.enable_q = False\n",
    "\n",
    "    # quantized (modified pytorch doc implementation -> fixed layered input)\n",
    "    def forward(self, x):\n",
    "        hx = None\n",
    "        if self.rnn.batch_first:\n",
    "            x = x.transpose(0, 1)\n",
    "        seq_len, batch_size, _ = x.size()\n",
    "\n",
    "        if hx is None:\n",
    "            hx = torch.zeros(self.rnn.num_layers, batch_size, self.rnn.hidden_size, device=x.device)\n",
    "        h_t_minus_1 = hx.detach()\n",
    "        h_t = torch.zeros_like(h_t_minus_1)\n",
    "\n",
    "        output = []\n",
    "\n",
    "        # Quantize inputs\n",
    "        if self.enable_q:\n",
    "            x_quantized, scale = quantize(x.clone(), self.bits)\n",
    "            x = dequantize(x_quantized, scale).requires_grad_()\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            h_t_new = []\n",
    "            for layer in range(self.rnn.num_layers):\n",
    "                weight_ih = getattr(self.rnn, f'weight_ih_l{layer}')\n",
    "                bias_ih = getattr(self.rnn, f'bias_ih_l{layer}')\n",
    "                weight_hh = getattr(self.rnn, f'weight_hh_l{layer}')\n",
    "                bias_hh = getattr(self.rnn, f'bias_hh_l{layer}')\n",
    "\n",
    "                xin = x[t] if layer == 0 else h_t[layer-1]\n",
    "\n",
    "                h_layer = torch.tanh(\n",
    "                    xin @ weight_ih.T\n",
    "                    + bias_ih\n",
    "                    + h_t_minus_1[layer] @ weight_hh.T\n",
    "                    + bias_hh\n",
    "                )\n",
    "\n",
    "                # Quantize hidden state\n",
    "                if self.enable_q:\n",
    "                    ht_quantized, scale = quantize(h_layer, self.bits)\n",
    "                    h_layer = dequantize(ht_quantized, scale).requires_grad_()\n",
    "\n",
    "                h_t_new.append(h_layer)\n",
    "\n",
    "            h_t = torch.stack(h_t_new)\n",
    "            output.append(h_t[-1])\n",
    "\n",
    "            h_t_minus_1 = h_t.detach()\n",
    "\n",
    "        output = torch.stack(output)\n",
    "        if self.rnn.batch_first:\n",
    "            output = output.transpose(0, 1)\n",
    "\n",
    "        out = self.fc(output[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pretrained model\n",
    "model_pre = FSDNN_RNN(args['ysize'], args['rnn_hidden'], args['rnn_layers'], args['rnn_outputs'])\n",
    "model_pre.load_state_dict(torch.load('chkpt_t1.pt', weights_only=True)) #load pretrained \n",
    "\n",
    "# \n",
    "model = FSDNN_RNN_Q(args['ysize'], args['rnn_hidden'], args['rnn_layers'], args['rnn_outputs'])\n",
    "pretrained_weights = model_pre.state_dict()\n",
    "new_model_dict = model.state_dict()\n",
    "pretrained_weights = {k: v for k, v in pretrained_weights.items() if k in new_model_dict}\n",
    "new_model_dict.update(pretrained_weights)\n",
    "model.load_state_dict(new_model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 weight_ih diff: 0.0\n",
      "Layer 0 weight_hh diff: 0.0\n",
      "Layer 1 weight_ih diff: 0.0\n",
      "Layer 1 weight_hh diff: 0.0\n",
      "Layer 2 weight_ih diff: 0.0\n",
      "Layer 2 weight_hh diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "for layer in range(model.rnn.num_layers):\n",
    "    print(f\"Layer {layer} weight_ih diff:\", torch.norm(\n",
    "        getattr(model.rnn, f'weight_ih_l{layer}') - getattr(model_pre.rnn, f'weight_ih_l{layer}')\n",
    "    ).item())\n",
    "    print(f\"Layer {layer} weight_hh diff:\", torch.norm(\n",
    "        getattr(model.rnn, f'weight_hh_l{layer}') - getattr(model_pre.rnn, f'weight_hh_l{layer}')\n",
    "    ).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model for an epoch\n",
    "def train_model(trainset,trainlabels,model,optimizer,criterion,**kwargs):\n",
    "    trainlen = trainset.shape[0]\n",
    "    nbatches = math.ceil(trainlen/kwargs['batch_size'])\n",
    "    if trainlen % kwargs['batch_size'] == 1:\n",
    "        nbatches -= 1\n",
    "    total_loss = 0\n",
    "    total_backs = 0\n",
    "    with tqdm(total=nbatches,disable=(kwargs['verbose']<2)) as pbar:\n",
    "        model = model.train()\n",
    "        for b in range(nbatches):\n",
    "\n",
    "            #Obtain batch\n",
    "            X = trainset[b*kwargs['batch_size']:min(trainlen,(b+1)*kwargs['batch_size'])].clone().float()\n",
    "            X = X.to(kwargs['device'])\n",
    "            Y = trainlabels[b*kwargs['batch_size']:min(trainlen,(b+1)*kwargs['batch_size'])].clone().long().to(kwargs['device'])\n",
    "            #import pdb; pdb.set_trace()\n",
    "\n",
    "            #Propagate\n",
    "            posteriors = model(X)\n",
    "\n",
    "            #Backpropagate\n",
    "            loss = criterion(posteriors,Y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Track loss\n",
    "            if total_backs == 100:\n",
    "                total_loss = total_loss*0.99+loss.detach().cpu().numpy()\n",
    "            else:\n",
    "                total_loss += loss.detach().cpu().numpy()\n",
    "                total_backs += 1\n",
    "            pbar.set_description(f'Training epoch. Loss {total_loss/(total_backs+1):.2f}')\n",
    "            pbar.update()\n",
    "    return total_loss/(total_backs+1)\n",
    "\n",
    "#Validate last epoch's model\n",
    "def validate_model(validset,validlabels,model,**kwargs):\n",
    "    validlen = validset.shape[0]\n",
    "    acc = 0\n",
    "    total = 0\n",
    "    nbatches = math.ceil(validlen/kwargs['batch_size'])\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=nbatches,disable=(kwargs['verbose']<2)) as pbar:\n",
    "            model = model.eval()\n",
    "            for b in range(nbatches):\n",
    "                #Obtain batch\n",
    "                X = validset[b*kwargs['batch_size']:min(validlen,(b+1)*kwargs['batch_size'])].clone().float().to(kwargs['device'])\n",
    "                Y = validlabels[b*kwargs['batch_size']:min(validlen,(b+1)*kwargs['batch_size'])].clone().long().to(kwargs['device'])\n",
    "                #Propagate\n",
    "                posteriors = model(X)\n",
    "                #Accumulate accuracy\n",
    "                estimated = torch.argmax(posteriors,dim=1)\n",
    "                acc += sum((estimated.cpu().numpy() == Y.cpu().numpy()))\n",
    "                total+=Y.shape[0]\n",
    "                pbar.set_description(f'Evaluating epoch. Accuracy {100*acc/total:.2f}%')\n",
    "                pbar.update()\n",
    "    return 100*acc/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.33333333333333\n",
      "90.33333333333333\n",
      "19.333333333333332\n"
     ]
    }
   ],
   "source": [
    "# validate equivalence\n",
    "acc = validate_model(testset,testlabels,model_pre,**args)\n",
    "print(acc)\n",
    "acc = validate_model(testset,testlabels,model,**args)\n",
    "print(acc)\n",
    "model.enable_q = True\n",
    "acc = validate_model(testset,testlabels,model,**args)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1000. Training loss: 2.67, Validation accuracy: 39.63%\n",
      "Epoch 2 of 1000. Training loss: 1.67, Validation accuracy: 45.93%\n",
      "Epoch 3 of 1000. Training loss: 1.54, Validation accuracy: 55.19%\n",
      "Epoch 4 of 1000. Training loss: 1.44, Validation accuracy: 53.70%\n",
      "Epoch 5 of 1000. Training loss: 1.32, Validation accuracy: 53.33%\n",
      "Epoch 6 of 1000. Training loss: 1.31, Validation accuracy: 56.30%\n",
      "Epoch 7 of 1000. Training loss: 1.34, Validation accuracy: 51.11%\n",
      "Epoch 8 of 1000. Training loss: 1.26, Validation accuracy: 55.56%\n",
      "Epoch 9 of 1000. Training loss: 1.20, Validation accuracy: 58.15%\n",
      "Epoch 10 of 1000. Training loss: 1.13, Validation accuracy: 59.63%\n",
      "Epoch 11 of 1000. Training loss: 1.12, Validation accuracy: 59.63%\n",
      "Epoch 12 of 1000. Training loss: 1.09, Validation accuracy: 60.74%\n",
      "Epoch 13 of 1000. Training loss: 1.11, Validation accuracy: 59.26%\n",
      "Epoch 14 of 1000. Training loss: 1.14, Validation accuracy: 59.63%\n",
      "Epoch 15 of 1000. Training loss: 1.13, Validation accuracy: 58.89%\n",
      "Epoch 16 of 1000. Training loss: 1.12, Validation accuracy: 64.07%\n",
      "Epoch 17 of 1000. Training loss: 1.05, Validation accuracy: 60.37%\n",
      "Epoch 18 of 1000. Training loss: 1.12, Validation accuracy: 52.59%\n",
      "Epoch 19 of 1000. Training loss: 1.16, Validation accuracy: 52.59%\n",
      "Epoch 20 of 1000. Training loss: 1.14, Validation accuracy: 57.04%\n",
      "Epoch 21 of 1000. Training loss: 1.18, Validation accuracy: 60.37%\n",
      "Epoch 22 of 1000. Training loss: 1.06, Validation accuracy: 61.85%\n",
      "Epoch 23 of 1000. Training loss: 1.05, Validation accuracy: 65.93%\n",
      "Epoch 24 of 1000. Training loss: 1.05, Validation accuracy: 68.15%\n",
      "Epoch 25 of 1000. Training loss: 1.04, Validation accuracy: 64.81%\n",
      "Epoch 26 of 1000. Training loss: 0.96, Validation accuracy: 65.93%\n",
      "Epoch 27 of 1000. Training loss: 0.93, Validation accuracy: 64.81%\n",
      "Epoch 28 of 1000. Training loss: 0.97, Validation accuracy: 67.04%\n",
      "Epoch 29 of 1000. Training loss: 0.91, Validation accuracy: 70.37%\n",
      "Epoch 30 of 1000. Training loss: 0.92, Validation accuracy: 72.59%\n",
      "Epoch 31 of 1000. Training loss: 0.96, Validation accuracy: 66.30%\n",
      "Epoch 32 of 1000. Training loss: 0.98, Validation accuracy: 69.26%\n",
      "Epoch 33 of 1000. Training loss: 1.06, Validation accuracy: 66.30%\n",
      "Epoch 34 of 1000. Training loss: 1.00, Validation accuracy: 72.96%\n",
      "Epoch 35 of 1000. Training loss: 0.95, Validation accuracy: 73.33%\n",
      "Epoch 36 of 1000. Training loss: 0.91, Validation accuracy: 67.78%\n",
      "Epoch 37 of 1000. Training loss: 0.98, Validation accuracy: 65.19%\n",
      "Epoch 38 of 1000. Training loss: 1.00, Validation accuracy: 67.78%\n",
      "Epoch 39 of 1000. Training loss: 0.91, Validation accuracy: 70.00%\n",
      "Epoch 40 of 1000. Training loss: 0.88, Validation accuracy: 66.30%\n",
      "Epoch 41 of 1000. Training loss: 0.92, Validation accuracy: 68.15%\n",
      "Epoch 42 of 1000. Training loss: 0.93, Validation accuracy: 69.63%\n",
      "Epoch 43 of 1000. Training loss: 0.96, Validation accuracy: 68.15%\n",
      "Epoch 44 of 1000. Training loss: 0.96, Validation accuracy: 68.52%\n",
      "Epoch 45 of 1000. Training loss: 0.93, Validation accuracy: 67.04%\n",
      "Epoch 46 of 1000. Training loss: 0.88, Validation accuracy: 65.56%\n",
      "Epoch 47 of 1000. Training loss: 0.90, Validation accuracy: 60.74%\n",
      "Epoch 48 of 1000. Training loss: 0.91, Validation accuracy: 68.15%\n",
      "Epoch 49 of 1000. Training loss: 0.89, Validation accuracy: 70.74%\n",
      "Epoch 50 of 1000. Training loss: 0.94, Validation accuracy: 68.52%\n",
      "Epoch 51 of 1000. Training loss: 0.94, Validation accuracy: 67.78%\n",
      "Epoch 52 of 1000. Training loss: 0.94, Validation accuracy: 73.33%\n",
      "Epoch 53 of 1000. Training loss: 0.92, Validation accuracy: 68.52%\n",
      "Epoch 54 of 1000. Training loss: 0.87, Validation accuracy: 69.26%\n",
      "Epoch 55 of 1000. Training loss: 0.84, Validation accuracy: 71.85%\n",
      "Epoch 56 of 1000. Training loss: 0.90, Validation accuracy: 71.85%\n",
      "Epoch 57 of 1000. Training loss: 0.91, Validation accuracy: 72.22%\n",
      "Epoch 58 of 1000. Training loss: 0.92, Validation accuracy: 69.63%\n",
      "Epoch 59 of 1000. Training loss: 0.88, Validation accuracy: 68.15%\n",
      "Epoch 60 of 1000. Training loss: 0.92, Validation accuracy: 64.81%\n",
      "Epoch 61 of 1000. Training loss: 0.93, Validation accuracy: 70.74%\n",
      "Epoch 62 of 1000. Training loss: 0.87, Validation accuracy: 72.96%\n",
      "Epoch 63 of 1000. Training loss: 0.85, Validation accuracy: 72.59%\n",
      "Epoch 64 of 1000. Training loss: 0.82, Validation accuracy: 72.59%\n",
      "Epoch 65 of 1000. Training loss: 0.88, Validation accuracy: 76.30%\n",
      "Epoch 66 of 1000. Training loss: 0.86, Validation accuracy: 64.44%\n",
      "Epoch 67 of 1000. Training loss: 0.91, Validation accuracy: 70.00%\n",
      "Epoch 68 of 1000. Training loss: 0.91, Validation accuracy: 68.89%\n",
      "Epoch 69 of 1000. Training loss: 0.88, Validation accuracy: 70.37%\n",
      "Epoch 70 of 1000. Training loss: 0.83, Validation accuracy: 62.59%\n",
      "Epoch 71 of 1000. Training loss: 0.87, Validation accuracy: 69.63%\n",
      "Epoch 72 of 1000. Training loss: 0.89, Validation accuracy: 70.74%\n",
      "Epoch 73 of 1000. Training loss: 0.83, Validation accuracy: 72.59%\n",
      "Epoch 74 of 1000. Training loss: 0.82, Validation accuracy: 69.26%\n",
      "Epoch 75 of 1000. Training loss: 0.85, Validation accuracy: 64.81%\n",
      "Epoch 76 of 1000. Training loss: 0.80, Validation accuracy: 71.11%\n",
      "Epoch 77 of 1000. Training loss: 0.77, Validation accuracy: 68.15%\n",
      "Epoch 78 of 1000. Training loss: 0.82, Validation accuracy: 71.48%\n",
      "Epoch 79 of 1000. Training loss: 0.81, Validation accuracy: 66.67%\n",
      "Epoch 80 of 1000. Training loss: 0.87, Validation accuracy: 62.59%\n",
      "Epoch 81 of 1000. Training loss: 0.79, Validation accuracy: 72.22%\n",
      "Epoch 82 of 1000. Training loss: 0.76, Validation accuracy: 68.52%\n",
      "Epoch 83 of 1000. Training loss: 0.82, Validation accuracy: 75.19%\n",
      "Epoch 84 of 1000. Training loss: 0.85, Validation accuracy: 70.37%\n",
      "Epoch 85 of 1000. Training loss: 0.82, Validation accuracy: 73.70%\n",
      "Epoch 86 of 1000. Training loss: 0.75, Validation accuracy: 68.52%\n",
      "Epoch 87 of 1000. Training loss: 0.81, Validation accuracy: 69.26%\n",
      "Epoch 88 of 1000. Training loss: 0.82, Validation accuracy: 67.04%\n",
      "Epoch 89 of 1000. Training loss: 0.82, Validation accuracy: 71.48%\n",
      "Epoch 90 of 1000. Training loss: 0.83, Validation accuracy: 71.85%\n",
      "Epoch 91 of 1000. Training loss: 0.80, Validation accuracy: 66.67%\n",
      "Epoch 92 of 1000. Training loss: 0.86, Validation accuracy: 69.63%\n",
      "Epoch 93 of 1000. Training loss: 0.87, Validation accuracy: 70.00%\n",
      "Epoch 94 of 1000. Training loss: 0.86, Validation accuracy: 67.04%\n",
      "Epoch 95 of 1000. Training loss: 0.80, Validation accuracy: 68.89%\n",
      "Epoch 96 of 1000. Training loss: 0.79, Validation accuracy: 67.41%\n",
      "Epoch 97 of 1000. Training loss: 0.89, Validation accuracy: 64.07%\n",
      "Epoch 98 of 1000. Training loss: 0.87, Validation accuracy: 66.67%\n",
      "Epoch 99 of 1000. Training loss: 0.80, Validation accuracy: 68.89%\n",
      "Epoch 100 of 1000. Training loss: 0.78, Validation accuracy: 70.37%\n",
      "Epoch 101 of 1000. Training loss: 0.73, Validation accuracy: 68.89%\n",
      "Epoch 102 of 1000. Training loss: 0.76, Validation accuracy: 67.41%\n",
      "Epoch 103 of 1000. Training loss: 0.76, Validation accuracy: 64.07%\n",
      "Epoch 104 of 1000. Training loss: 0.77, Validation accuracy: 68.89%\n",
      "Epoch 105 of 1000. Training loss: 0.80, Validation accuracy: 66.30%\n",
      "Epoch 106 of 1000. Training loss: 0.81, Validation accuracy: 64.44%\n",
      "Epoch 107 of 1000. Training loss: 0.81, Validation accuracy: 67.41%\n",
      "Epoch 108 of 1000. Training loss: 0.87, Validation accuracy: 70.74%\n",
      "Epoch 109 of 1000. Training loss: 0.79, Validation accuracy: 74.07%\n",
      "Epoch 110 of 1000. Training loss: 0.82, Validation accuracy: 71.11%\n",
      "Epoch 111 of 1000. Training loss: 0.78, Validation accuracy: 71.11%\n",
      "Epoch 112 of 1000. Training loss: 0.76, Validation accuracy: 70.00%\n",
      "Epoch 113 of 1000. Training loss: 0.79, Validation accuracy: 70.00%\n",
      "Epoch 114 of 1000. Training loss: 0.79, Validation accuracy: 71.11%\n",
      "Epoch 115 of 1000. Training loss: 0.81, Validation accuracy: 65.93%\n",
      "Epoch 116 of 1000. Training loss: 0.72, Validation accuracy: 67.41%\n",
      "Epoch 117 of 1000. Training loss: 0.89, Validation accuracy: 66.30%\n",
      "Epoch 118 of 1000. Training loss: 0.83, Validation accuracy: 76.30%\n",
      "Epoch 119 of 1000. Training loss: 0.75, Validation accuracy: 69.26%\n",
      "Epoch 120 of 1000. Training loss: 0.78, Validation accuracy: 71.48%\n",
      "Epoch 121 of 1000. Training loss: 0.79, Validation accuracy: 70.74%\n",
      "Epoch 122 of 1000. Training loss: 0.71, Validation accuracy: 73.33%\n",
      "Epoch 123 of 1000. Training loss: 0.73, Validation accuracy: 72.22%\n",
      "Epoch 124 of 1000. Training loss: 0.75, Validation accuracy: 70.74%\n",
      "Epoch 125 of 1000. Training loss: 0.79, Validation accuracy: 69.63%\n",
      "Epoch 126 of 1000. Training loss: 0.81, Validation accuracy: 64.81%\n",
      "Epoch 127 of 1000. Training loss: 0.69, Validation accuracy: 71.11%\n",
      "Epoch 128 of 1000. Training loss: 0.79, Validation accuracy: 73.33%\n",
      "Epoch 129 of 1000. Training loss: 0.79, Validation accuracy: 71.85%\n",
      "Epoch 130 of 1000. Training loss: 0.78, Validation accuracy: 71.11%\n",
      "Epoch 131 of 1000. Training loss: 0.71, Validation accuracy: 67.41%\n",
      "Epoch 132 of 1000. Training loss: 0.65, Validation accuracy: 72.59%\n",
      "Epoch 133 of 1000. Training loss: 0.78, Validation accuracy: 64.81%\n",
      "Epoch 134 of 1000. Training loss: 0.78, Validation accuracy: 72.22%\n",
      "Epoch 135 of 1000. Training loss: 0.71, Validation accuracy: 70.37%\n",
      "Epoch 136 of 1000. Training loss: 0.75, Validation accuracy: 75.93%\n",
      "Epoch 137 of 1000. Training loss: 0.69, Validation accuracy: 65.56%\n",
      "Epoch 138 of 1000. Training loss: 0.87, Validation accuracy: 71.11%\n",
      "Epoch 139 of 1000. Training loss: 0.82, Validation accuracy: 67.41%\n",
      "Epoch 140 of 1000. Training loss: 0.75, Validation accuracy: 72.22%\n",
      "Epoch 141 of 1000. Training loss: 0.81, Validation accuracy: 69.26%\n",
      "Epoch 142 of 1000. Training loss: 0.78, Validation accuracy: 68.15%\n",
      "Epoch 143 of 1000. Training loss: 0.74, Validation accuracy: 67.78%\n",
      "Epoch 144 of 1000. Training loss: 0.75, Validation accuracy: 72.22%\n",
      "Epoch 145 of 1000. Training loss: 0.72, Validation accuracy: 72.96%\n",
      "Epoch 146 of 1000. Training loss: 0.79, Validation accuracy: 73.33%\n",
      "Epoch 147 of 1000. Training loss: 0.75, Validation accuracy: 70.00%\n",
      "Epoch 148 of 1000. Training loss: 0.73, Validation accuracy: 71.48%\n",
      "Epoch 149 of 1000. Training loss: 0.74, Validation accuracy: 66.30%\n",
      "Epoch 150 of 1000. Training loss: 0.72, Validation accuracy: 70.00%\n",
      "Epoch 151 of 1000. Training loss: 0.72, Validation accuracy: 70.00%\n",
      "Epoch 152 of 1000. Training loss: 0.67, Validation accuracy: 74.44%\n",
      "Epoch 153 of 1000. Training loss: 0.68, Validation accuracy: 72.96%\n",
      "Epoch 154 of 1000. Training loss: 0.71, Validation accuracy: 68.52%\n",
      "Epoch 155 of 1000. Training loss: 0.69, Validation accuracy: 72.59%\n",
      "Epoch 156 of 1000. Training loss: 0.70, Validation accuracy: 70.37%\n",
      "Epoch 157 of 1000. Training loss: 0.84, Validation accuracy: 70.00%\n",
      "Epoch 158 of 1000. Training loss: 0.74, Validation accuracy: 76.67%\n",
      "Epoch 159 of 1000. Training loss: 0.64, Validation accuracy: 74.44%\n",
      "Epoch 160 of 1000. Training loss: 0.70, Validation accuracy: 72.96%\n",
      "Epoch 161 of 1000. Training loss: 0.72, Validation accuracy: 69.26%\n",
      "Epoch 162 of 1000. Training loss: 0.72, Validation accuracy: 63.70%\n",
      "Epoch 163 of 1000. Training loss: 0.72, Validation accuracy: 72.22%\n",
      "Epoch 164 of 1000. Training loss: 0.78, Validation accuracy: 67.78%\n",
      "Epoch 165 of 1000. Training loss: 0.73, Validation accuracy: 65.93%\n",
      "Epoch 166 of 1000. Training loss: 0.72, Validation accuracy: 65.93%\n",
      "Epoch 167 of 1000. Training loss: 0.71, Validation accuracy: 70.00%\n",
      "Epoch 168 of 1000. Training loss: 0.74, Validation accuracy: 69.63%\n",
      "Epoch 169 of 1000. Training loss: 0.73, Validation accuracy: 73.33%\n",
      "Epoch 170 of 1000. Training loss: 0.75, Validation accuracy: 69.63%\n",
      "Epoch 171 of 1000. Training loss: 0.68, Validation accuracy: 72.59%\n",
      "Epoch 172 of 1000. Training loss: 0.71, Validation accuracy: 71.85%\n",
      "Epoch 173 of 1000. Training loss: 0.72, Validation accuracy: 67.78%\n",
      "Epoch 174 of 1000. Training loss: 0.71, Validation accuracy: 68.89%\n",
      "Epoch 175 of 1000. Training loss: 0.78, Validation accuracy: 69.63%\n",
      "Epoch 176 of 1000. Training loss: 0.75, Validation accuracy: 72.22%\n",
      "Epoch 177 of 1000. Training loss: 0.77, Validation accuracy: 66.67%\n",
      "Epoch 178 of 1000. Training loss: 0.73, Validation accuracy: 73.33%\n",
      "Epoch 179 of 1000. Training loss: 0.72, Validation accuracy: 72.22%\n",
      "Epoch 180 of 1000. Training loss: 0.78, Validation accuracy: 70.37%\n",
      "Epoch 181 of 1000. Training loss: 0.69, Validation accuracy: 72.59%\n",
      "Epoch 182 of 1000. Training loss: 0.75, Validation accuracy: 69.26%\n",
      "Epoch 183 of 1000. Training loss: 0.67, Validation accuracy: 73.70%\n",
      "Epoch 184 of 1000. Training loss: 0.68, Validation accuracy: 71.85%\n",
      "Epoch 185 of 1000. Training loss: 0.70, Validation accuracy: 72.59%\n",
      "Epoch 186 of 1000. Training loss: 0.66, Validation accuracy: 69.26%\n",
      "Epoch 187 of 1000. Training loss: 0.77, Validation accuracy: 74.44%\n",
      "Epoch 188 of 1000. Training loss: 0.71, Validation accuracy: 73.33%\n",
      "Epoch 189 of 1000. Training loss: 0.62, Validation accuracy: 76.30%\n",
      "Epoch 190 of 1000. Training loss: 0.77, Validation accuracy: 69.63%\n",
      "Epoch 191 of 1000. Training loss: 0.76, Validation accuracy: 71.48%\n",
      "Epoch 192 of 1000. Training loss: 0.76, Validation accuracy: 70.74%\n",
      "Epoch 193 of 1000. Training loss: 0.75, Validation accuracy: 67.78%\n",
      "Epoch 194 of 1000. Training loss: 0.74, Validation accuracy: 69.63%\n",
      "Epoch 195 of 1000. Training loss: 0.75, Validation accuracy: 74.81%\n",
      "Epoch 196 of 1000. Training loss: 0.83, Validation accuracy: 69.26%\n",
      "Epoch 197 of 1000. Training loss: 0.79, Validation accuracy: 68.89%\n",
      "Epoch 198 of 1000. Training loss: 0.74, Validation accuracy: 70.00%\n",
      "Epoch 199 of 1000. Training loss: 0.79, Validation accuracy: 68.52%\n",
      "Epoch 200 of 1000. Training loss: 0.74, Validation accuracy: 70.74%\n",
      "Epoch 201 of 1000. Training loss: 0.66, Validation accuracy: 71.85%\n",
      "Epoch 202 of 1000. Training loss: 0.60, Validation accuracy: 73.33%\n",
      "Epoch 203 of 1000. Training loss: 0.58, Validation accuracy: 76.67%\n",
      "Epoch 204 of 1000. Training loss: 0.60, Validation accuracy: 74.07%\n",
      "Epoch 205 of 1000. Training loss: 0.58, Validation accuracy: 72.96%\n",
      "Epoch 206 of 1000. Training loss: 0.56, Validation accuracy: 72.59%\n",
      "Epoch 207 of 1000. Training loss: 0.57, Validation accuracy: 75.19%\n",
      "Epoch 208 of 1000. Training loss: 0.56, Validation accuracy: 76.30%\n",
      "Epoch 209 of 1000. Training loss: 0.57, Validation accuracy: 73.70%\n",
      "Epoch 210 of 1000. Training loss: 0.54, Validation accuracy: 72.96%\n",
      "Epoch 211 of 1000. Training loss: 0.55, Validation accuracy: 76.30%\n",
      "Epoch 212 of 1000. Training loss: 0.55, Validation accuracy: 76.30%\n",
      "Epoch 213 of 1000. Training loss: 0.54, Validation accuracy: 75.93%\n",
      "Epoch 214 of 1000. Training loss: 0.53, Validation accuracy: 76.30%\n",
      "Epoch 215 of 1000. Training loss: 0.54, Validation accuracy: 74.07%\n",
      "Epoch 216 of 1000. Training loss: 0.50, Validation accuracy: 77.41%\n",
      "Epoch 217 of 1000. Training loss: 0.51, Validation accuracy: 75.19%\n",
      "Epoch 218 of 1000. Training loss: 0.56, Validation accuracy: 71.48%\n",
      "Epoch 219 of 1000. Training loss: 0.56, Validation accuracy: 74.81%\n",
      "Epoch 220 of 1000. Training loss: 0.58, Validation accuracy: 77.78%\n",
      "Epoch 221 of 1000. Training loss: 0.54, Validation accuracy: 80.74%\n",
      "Epoch 222 of 1000. Training loss: 0.53, Validation accuracy: 77.41%\n",
      "Epoch 223 of 1000. Training loss: 0.50, Validation accuracy: 80.74%\n",
      "Epoch 224 of 1000. Training loss: 0.49, Validation accuracy: 79.63%\n",
      "Epoch 225 of 1000. Training loss: 0.48, Validation accuracy: 78.52%\n",
      "Epoch 226 of 1000. Training loss: 0.50, Validation accuracy: 77.41%\n",
      "Epoch 227 of 1000. Training loss: 0.51, Validation accuracy: 78.89%\n",
      "Epoch 228 of 1000. Training loss: 0.51, Validation accuracy: 75.56%\n",
      "Epoch 229 of 1000. Training loss: 0.51, Validation accuracy: 75.19%\n",
      "Epoch 230 of 1000. Training loss: 0.53, Validation accuracy: 78.89%\n",
      "Epoch 231 of 1000. Training loss: 0.54, Validation accuracy: 74.81%\n",
      "Epoch 232 of 1000. Training loss: 0.50, Validation accuracy: 78.15%\n",
      "Epoch 233 of 1000. Training loss: 0.49, Validation accuracy: 77.41%\n",
      "Epoch 234 of 1000. Training loss: 0.46, Validation accuracy: 75.56%\n",
      "Epoch 235 of 1000. Training loss: 0.48, Validation accuracy: 74.81%\n",
      "Epoch 236 of 1000. Training loss: 0.51, Validation accuracy: 73.33%\n",
      "Epoch 237 of 1000. Training loss: 0.49, Validation accuracy: 78.15%\n",
      "Epoch 238 of 1000. Training loss: 0.49, Validation accuracy: 81.11%\n",
      "Epoch 239 of 1000. Training loss: 0.51, Validation accuracy: 80.37%\n",
      "Epoch 240 of 1000. Training loss: 0.48, Validation accuracy: 77.41%\n",
      "Epoch 241 of 1000. Training loss: 0.48, Validation accuracy: 77.41%\n",
      "Epoch 242 of 1000. Training loss: 0.45, Validation accuracy: 76.30%\n",
      "Epoch 243 of 1000. Training loss: 0.46, Validation accuracy: 80.37%\n",
      "Epoch 244 of 1000. Training loss: 0.46, Validation accuracy: 77.78%\n",
      "Epoch 245 of 1000. Training loss: 0.46, Validation accuracy: 78.15%\n",
      "Epoch 246 of 1000. Training loss: 0.43, Validation accuracy: 78.89%\n",
      "Epoch 247 of 1000. Training loss: 0.49, Validation accuracy: 75.19%\n",
      "Epoch 248 of 1000. Training loss: 0.45, Validation accuracy: 75.93%\n",
      "Epoch 249 of 1000. Training loss: 0.46, Validation accuracy: 79.63%\n",
      "Epoch 250 of 1000. Training loss: 0.50, Validation accuracy: 74.44%\n",
      "Epoch 251 of 1000. Training loss: 0.55, Validation accuracy: 77.78%\n",
      "Epoch 252 of 1000. Training loss: 0.50, Validation accuracy: 80.37%\n",
      "Epoch 253 of 1000. Training loss: 0.49, Validation accuracy: 74.07%\n",
      "Epoch 254 of 1000. Training loss: 0.49, Validation accuracy: 77.04%\n",
      "Epoch 255 of 1000. Training loss: 0.47, Validation accuracy: 78.89%\n",
      "Epoch 256 of 1000. Training loss: 0.46, Validation accuracy: 77.41%\n",
      "Epoch 257 of 1000. Training loss: 0.46, Validation accuracy: 76.67%\n",
      "Epoch 258 of 1000. Training loss: 0.46, Validation accuracy: 77.78%\n",
      "Epoch 259 of 1000. Training loss: 0.49, Validation accuracy: 76.67%\n",
      "Epoch 260 of 1000. Training loss: 0.45, Validation accuracy: 80.00%\n",
      "Epoch 261 of 1000. Training loss: 0.45, Validation accuracy: 80.37%\n",
      "Epoch 262 of 1000. Training loss: 0.41, Validation accuracy: 77.04%\n",
      "Epoch 263 of 1000. Training loss: 0.43, Validation accuracy: 81.48%\n",
      "Epoch 264 of 1000. Training loss: 0.42, Validation accuracy: 79.26%\n",
      "Epoch 265 of 1000. Training loss: 0.44, Validation accuracy: 75.19%\n",
      "Epoch 266 of 1000. Training loss: 0.45, Validation accuracy: 77.78%\n",
      "Epoch 267 of 1000. Training loss: 0.50, Validation accuracy: 76.30%\n",
      "Epoch 268 of 1000. Training loss: 0.45, Validation accuracy: 76.67%\n",
      "Epoch 269 of 1000. Training loss: 0.41, Validation accuracy: 83.33%\n",
      "Epoch 270 of 1000. Training loss: 0.43, Validation accuracy: 79.63%\n",
      "Epoch 271 of 1000. Training loss: 0.44, Validation accuracy: 77.41%\n",
      "Epoch 272 of 1000. Training loss: 0.43, Validation accuracy: 76.30%\n",
      "Epoch 273 of 1000. Training loss: 0.44, Validation accuracy: 79.26%\n",
      "Epoch 274 of 1000. Training loss: 0.43, Validation accuracy: 74.81%\n",
      "Epoch 275 of 1000. Training loss: 0.44, Validation accuracy: 79.63%\n",
      "Epoch 276 of 1000. Training loss: 0.43, Validation accuracy: 79.26%\n",
      "Epoch 277 of 1000. Training loss: 0.48, Validation accuracy: 74.81%\n",
      "Epoch 278 of 1000. Training loss: 0.43, Validation accuracy: 76.30%\n",
      "Epoch 279 of 1000. Training loss: 0.46, Validation accuracy: 75.19%\n",
      "Epoch 280 of 1000. Training loss: 0.46, Validation accuracy: 78.89%\n",
      "Epoch 281 of 1000. Training loss: 0.46, Validation accuracy: 73.70%\n",
      "Epoch 282 of 1000. Training loss: 0.53, Validation accuracy: 75.56%\n",
      "Epoch 283 of 1000. Training loss: 0.46, Validation accuracy: 77.41%\n",
      "Epoch 284 of 1000. Training loss: 0.44, Validation accuracy: 76.30%\n",
      "Epoch 285 of 1000. Training loss: 0.42, Validation accuracy: 78.89%\n",
      "Epoch 286 of 1000. Training loss: 0.43, Validation accuracy: 76.67%\n",
      "Epoch 287 of 1000. Training loss: 0.42, Validation accuracy: 75.56%\n",
      "Epoch 288 of 1000. Training loss: 0.45, Validation accuracy: 77.41%\n",
      "Epoch 289 of 1000. Training loss: 0.45, Validation accuracy: 75.19%\n",
      "Epoch 290 of 1000. Training loss: 0.48, Validation accuracy: 75.19%\n",
      "Epoch 291 of 1000. Training loss: 0.45, Validation accuracy: 76.67%\n",
      "Epoch 292 of 1000. Training loss: 0.40, Validation accuracy: 75.93%\n",
      "Epoch 293 of 1000. Training loss: 0.41, Validation accuracy: 74.81%\n",
      "Epoch 294 of 1000. Training loss: 0.40, Validation accuracy: 77.78%\n",
      "Epoch 295 of 1000. Training loss: 0.44, Validation accuracy: 78.89%\n",
      "Epoch 296 of 1000. Training loss: 0.41, Validation accuracy: 78.15%\n",
      "Epoch 297 of 1000. Training loss: 0.43, Validation accuracy: 78.89%\n",
      "Epoch 298 of 1000. Training loss: 0.41, Validation accuracy: 80.00%\n",
      "Epoch 299 of 1000. Training loss: 0.43, Validation accuracy: 77.41%\n",
      "Epoch 300 of 1000. Training loss: 0.44, Validation accuracy: 77.78%\n",
      "Epoch 301 of 1000. Training loss: 0.42, Validation accuracy: 75.93%\n",
      "Epoch 302 of 1000. Training loss: 0.43, Validation accuracy: 75.93%\n",
      "Epoch 303 of 1000. Training loss: 0.40, Validation accuracy: 77.41%\n",
      "Epoch 304 of 1000. Training loss: 0.39, Validation accuracy: 77.04%\n",
      "Epoch 305 of 1000. Training loss: 0.39, Validation accuracy: 77.04%\n",
      "Epoch 306 of 1000. Training loss: 0.38, Validation accuracy: 74.07%\n",
      "Epoch 307 of 1000. Training loss: 0.39, Validation accuracy: 74.81%\n",
      "Epoch 308 of 1000. Training loss: 0.42, Validation accuracy: 77.41%\n",
      "Epoch 309 of 1000. Training loss: 0.45, Validation accuracy: 77.04%\n",
      "Epoch 310 of 1000. Training loss: 0.41, Validation accuracy: 76.30%\n",
      "Epoch 311 of 1000. Training loss: 0.47, Validation accuracy: 79.26%\n",
      "Epoch 312 of 1000. Training loss: 0.44, Validation accuracy: 77.04%\n",
      "Epoch 313 of 1000. Training loss: 0.39, Validation accuracy: 76.67%\n",
      "Epoch 314 of 1000. Training loss: 0.37, Validation accuracy: 75.93%\n",
      "Epoch 315 of 1000. Training loss: 0.41, Validation accuracy: 76.67%\n",
      "Epoch 316 of 1000. Training loss: 0.42, Validation accuracy: 76.30%\n",
      "Epoch 317 of 1000. Training loss: 0.42, Validation accuracy: 75.19%\n",
      "Epoch 318 of 1000. Training loss: 0.39, Validation accuracy: 76.30%\n",
      "Epoch 319 of 1000. Training loss: 0.41, Validation accuracy: 73.70%\n",
      "Epoch 320 of 1000. Training loss: 0.39, Validation accuracy: 76.67%\n",
      "Epoch 321 of 1000. Training loss: 0.39, Validation accuracy: 73.33%\n",
      "Epoch 322 of 1000. Training loss: 0.39, Validation accuracy: 72.96%\n",
      "Epoch 323 of 1000. Training loss: 0.40, Validation accuracy: 76.67%\n",
      "Epoch 324 of 1000. Training loss: 0.42, Validation accuracy: 75.93%\n",
      "Epoch 325 of 1000. Training loss: 0.39, Validation accuracy: 75.56%\n",
      "Epoch 326 of 1000. Training loss: 0.37, Validation accuracy: 78.52%\n",
      "Epoch 327 of 1000. Training loss: 0.36, Validation accuracy: 80.00%\n",
      "Epoch 328 of 1000. Training loss: 0.40, Validation accuracy: 74.07%\n",
      "Epoch 329 of 1000. Training loss: 0.38, Validation accuracy: 75.93%\n",
      "Epoch 330 of 1000. Training loss: 0.47, Validation accuracy: 81.11%\n",
      "Epoch 331 of 1000. Training loss: 0.41, Validation accuracy: 79.26%\n",
      "Epoch 332 of 1000. Training loss: 0.36, Validation accuracy: 77.41%\n",
      "Epoch 333 of 1000. Training loss: 0.35, Validation accuracy: 77.41%\n",
      "Epoch 334 of 1000. Training loss: 0.40, Validation accuracy: 75.19%\n",
      "Epoch 335 of 1000. Training loss: 0.36, Validation accuracy: 75.93%\n",
      "Epoch 336 of 1000. Training loss: 0.42, Validation accuracy: 75.56%\n",
      "Epoch 337 of 1000. Training loss: 0.45, Validation accuracy: 77.78%\n",
      "Epoch 338 of 1000. Training loss: 0.43, Validation accuracy: 74.81%\n",
      "Epoch 339 of 1000. Training loss: 0.39, Validation accuracy: 74.81%\n",
      "Epoch 340 of 1000. Training loss: 0.42, Validation accuracy: 77.41%\n",
      "Epoch 341 of 1000. Training loss: 0.44, Validation accuracy: 77.04%\n",
      "Epoch 342 of 1000. Training loss: 0.37, Validation accuracy: 74.81%\n",
      "Epoch 343 of 1000. Training loss: 0.39, Validation accuracy: 73.70%\n",
      "Epoch 344 of 1000. Training loss: 0.39, Validation accuracy: 72.59%\n",
      "Epoch 345 of 1000. Training loss: 0.41, Validation accuracy: 75.56%\n",
      "Epoch 346 of 1000. Training loss: 0.44, Validation accuracy: 71.48%\n",
      "Epoch 347 of 1000. Training loss: 0.42, Validation accuracy: 73.70%\n",
      "Epoch 348 of 1000. Training loss: 0.39, Validation accuracy: 68.52%\n",
      "Epoch 349 of 1000. Training loss: 0.44, Validation accuracy: 77.41%\n",
      "Epoch 350 of 1000. Training loss: 0.37, Validation accuracy: 72.96%\n",
      "Epoch 351 of 1000. Training loss: 0.35, Validation accuracy: 75.93%\n",
      "Epoch 352 of 1000. Training loss: 0.37, Validation accuracy: 77.04%\n",
      "Epoch 353 of 1000. Training loss: 0.40, Validation accuracy: 74.44%\n",
      "Epoch 354 of 1000. Training loss: 0.42, Validation accuracy: 72.22%\n",
      "Epoch 355 of 1000. Training loss: 0.43, Validation accuracy: 73.33%\n",
      "Epoch 356 of 1000. Training loss: 0.43, Validation accuracy: 75.93%\n",
      "Epoch 357 of 1000. Training loss: 0.36, Validation accuracy: 75.93%\n",
      "Epoch 358 of 1000. Training loss: 0.38, Validation accuracy: 76.67%\n",
      "Epoch 359 of 1000. Training loss: 0.36, Validation accuracy: 75.93%\n",
      "Epoch 360 of 1000. Training loss: 0.34, Validation accuracy: 75.19%\n",
      "Epoch 361 of 1000. Training loss: 0.35, Validation accuracy: 76.67%\n",
      "Epoch 362 of 1000. Training loss: 0.37, Validation accuracy: 74.44%\n",
      "Epoch 363 of 1000. Training loss: 0.38, Validation accuracy: 74.81%\n",
      "Epoch 364 of 1000. Training loss: 0.39, Validation accuracy: 73.70%\n",
      "Epoch 365 of 1000. Training loss: 0.42, Validation accuracy: 72.96%\n",
      "Epoch 366 of 1000. Training loss: 0.46, Validation accuracy: 74.44%\n",
      "Epoch 367 of 1000. Training loss: 0.42, Validation accuracy: 77.41%\n",
      "Epoch 368 of 1000. Training loss: 0.37, Validation accuracy: 79.26%\n",
      "Epoch 369 of 1000. Training loss: 0.39, Validation accuracy: 73.33%\n",
      "Epoch 370 of 1000. Training loss: 0.41, Validation accuracy: 76.30%\n",
      "Epoch 371 of 1000. Training loss: 0.36, Validation accuracy: 75.93%\n",
      "Epoch 372 of 1000. Training loss: 0.36, Validation accuracy: 73.70%\n",
      "Epoch 373 of 1000. Training loss: 0.37, Validation accuracy: 76.30%\n",
      "Epoch 374 of 1000. Training loss: 0.39, Validation accuracy: 72.22%\n",
      "Epoch 375 of 1000. Training loss: 0.45, Validation accuracy: 74.44%\n",
      "Epoch 376 of 1000. Training loss: 0.43, Validation accuracy: 73.33%\n",
      "Epoch 377 of 1000. Training loss: 0.34, Validation accuracy: 74.81%\n",
      "Epoch 378 of 1000. Training loss: 0.35, Validation accuracy: 77.04%\n",
      "Epoch 379 of 1000. Training loss: 0.37, Validation accuracy: 72.22%\n",
      "Epoch 380 of 1000. Training loss: 0.38, Validation accuracy: 73.33%\n",
      "Epoch 381 of 1000. Training loss: 0.40, Validation accuracy: 75.56%\n",
      "Epoch 382 of 1000. Training loss: 0.42, Validation accuracy: 74.44%\n",
      "Epoch 383 of 1000. Training loss: 0.37, Validation accuracy: 78.15%\n",
      "Epoch 384 of 1000. Training loss: 0.33, Validation accuracy: 73.70%\n",
      "Epoch 385 of 1000. Training loss: 0.36, Validation accuracy: 74.07%\n",
      "Epoch 386 of 1000. Training loss: 0.33, Validation accuracy: 75.56%\n",
      "Epoch 387 of 1000. Training loss: 0.35, Validation accuracy: 77.41%\n",
      "Epoch 388 of 1000. Training loss: 0.37, Validation accuracy: 76.67%\n",
      "Epoch 389 of 1000. Training loss: 0.33, Validation accuracy: 74.44%\n",
      "Epoch 390 of 1000. Training loss: 0.33, Validation accuracy: 75.93%\n",
      "Epoch 391 of 1000. Training loss: 0.37, Validation accuracy: 74.07%\n",
      "Epoch 392 of 1000. Training loss: 0.40, Validation accuracy: 72.59%\n",
      "Epoch 393 of 1000. Training loss: 0.38, Validation accuracy: 75.56%\n",
      "Epoch 394 of 1000. Training loss: 0.34, Validation accuracy: 78.89%\n",
      "Epoch 395 of 1000. Training loss: 0.34, Validation accuracy: 77.41%\n",
      "Epoch 396 of 1000. Training loss: 0.38, Validation accuracy: 73.33%\n",
      "Epoch 397 of 1000. Training loss: 0.36, Validation accuracy: 74.81%\n",
      "Epoch 398 of 1000. Training loss: 0.38, Validation accuracy: 74.44%\n",
      "Epoch 399 of 1000. Training loss: 0.38, Validation accuracy: 74.44%\n",
      "Epoch 400 of 1000. Training loss: 0.32, Validation accuracy: 75.56%\n",
      "Epoch 401 of 1000. Training loss: 0.29, Validation accuracy: 75.56%\n",
      "Epoch 402 of 1000. Training loss: 0.28, Validation accuracy: 77.78%\n",
      "Epoch 403 of 1000. Training loss: 0.27, Validation accuracy: 77.41%\n",
      "Epoch 404 of 1000. Training loss: 0.27, Validation accuracy: 75.19%\n",
      "Epoch 405 of 1000. Training loss: 0.28, Validation accuracy: 77.04%\n",
      "Epoch 406 of 1000. Training loss: 0.26, Validation accuracy: 74.81%\n",
      "Epoch 407 of 1000. Training loss: 0.26, Validation accuracy: 74.07%\n",
      "Epoch 408 of 1000. Training loss: 0.27, Validation accuracy: 73.70%\n",
      "Epoch 409 of 1000. Training loss: 0.26, Validation accuracy: 76.67%\n",
      "Epoch 410 of 1000. Training loss: 0.26, Validation accuracy: 74.07%\n",
      "Epoch 411 of 1000. Training loss: 0.27, Validation accuracy: 74.81%\n",
      "Epoch 412 of 1000. Training loss: 0.24, Validation accuracy: 72.59%\n",
      "Epoch 413 of 1000. Training loss: 0.26, Validation accuracy: 72.96%\n",
      "Epoch 414 of 1000. Training loss: 0.24, Validation accuracy: 74.07%\n",
      "Epoch 415 of 1000. Training loss: 0.26, Validation accuracy: 72.96%\n",
      "Epoch 416 of 1000. Training loss: 0.26, Validation accuracy: 75.93%\n",
      "Epoch 417 of 1000. Training loss: 0.25, Validation accuracy: 77.41%\n",
      "Epoch 418 of 1000. Training loss: 0.25, Validation accuracy: 76.30%\n",
      "Epoch 419 of 1000. Training loss: 0.24, Validation accuracy: 73.33%\n",
      "Epoch 420 of 1000. Training loss: 0.25, Validation accuracy: 74.07%\n",
      "Epoch 421 of 1000. Training loss: 0.27, Validation accuracy: 74.44%\n",
      "Epoch 422 of 1000. Training loss: 0.26, Validation accuracy: 74.81%\n",
      "Epoch 423 of 1000. Training loss: 0.25, Validation accuracy: 75.56%\n",
      "Epoch 424 of 1000. Training loss: 0.26, Validation accuracy: 75.56%\n",
      "Epoch 425 of 1000. Training loss: 0.25, Validation accuracy: 73.33%\n",
      "Epoch 426 of 1000. Training loss: 0.25, Validation accuracy: 72.22%\n",
      "Epoch 427 of 1000. Training loss: 0.23, Validation accuracy: 77.41%\n",
      "Epoch 428 of 1000. Training loss: 0.26, Validation accuracy: 73.70%\n",
      "Epoch 429 of 1000. Training loss: 0.24, Validation accuracy: 73.33%\n",
      "Epoch 430 of 1000. Training loss: 0.25, Validation accuracy: 73.33%\n",
      "Epoch 431 of 1000. Training loss: 0.24, Validation accuracy: 74.07%\n",
      "Epoch 432 of 1000. Training loss: 0.25, Validation accuracy: 74.81%\n",
      "Epoch 433 of 1000. Training loss: 0.26, Validation accuracy: 75.93%\n",
      "Epoch 434 of 1000. Training loss: 0.23, Validation accuracy: 75.93%\n",
      "Epoch 435 of 1000. Training loss: 0.23, Validation accuracy: 74.44%\n",
      "Epoch 436 of 1000. Training loss: 0.23, Validation accuracy: 74.81%\n",
      "Epoch 437 of 1000. Training loss: 0.23, Validation accuracy: 75.93%\n",
      "Epoch 438 of 1000. Training loss: 0.22, Validation accuracy: 75.56%\n",
      "Epoch 439 of 1000. Training loss: 0.21, Validation accuracy: 74.81%\n",
      "Epoch 440 of 1000. Training loss: 0.22, Validation accuracy: 74.44%\n",
      "Epoch 441 of 1000. Training loss: 0.23, Validation accuracy: 72.96%\n",
      "Epoch 442 of 1000. Training loss: 0.23, Validation accuracy: 74.81%\n",
      "Epoch 443 of 1000. Training loss: 0.23, Validation accuracy: 74.44%\n",
      "Epoch 444 of 1000. Training loss: 0.23, Validation accuracy: 74.44%\n",
      "Epoch 445 of 1000. Training loss: 0.23, Validation accuracy: 75.93%\n",
      "Epoch 446 of 1000. Training loss: 0.22, Validation accuracy: 74.44%\n",
      "Epoch 447 of 1000. Training loss: 0.23, Validation accuracy: 75.19%\n",
      "Epoch 448 of 1000. Training loss: 0.23, Validation accuracy: 75.56%\n",
      "Epoch 449 of 1000. Training loss: 0.23, Validation accuracy: 73.33%\n",
      "Epoch 450 of 1000. Training loss: 0.26, Validation accuracy: 71.85%\n",
      "Epoch 451 of 1000. Training loss: 0.28, Validation accuracy: 75.93%\n",
      "Epoch 452 of 1000. Training loss: 0.24, Validation accuracy: 75.19%\n",
      "Epoch 453 of 1000. Training loss: 0.22, Validation accuracy: 73.33%\n",
      "Epoch 454 of 1000. Training loss: 0.20, Validation accuracy: 75.93%\n",
      "Epoch 455 of 1000. Training loss: 0.20, Validation accuracy: 73.70%\n",
      "Epoch 456 of 1000. Training loss: 0.22, Validation accuracy: 73.70%\n",
      "Epoch 457 of 1000. Training loss: 0.26, Validation accuracy: 72.59%\n",
      "Epoch 458 of 1000. Training loss: 0.24, Validation accuracy: 75.19%\n",
      "Epoch 459 of 1000. Training loss: 0.25, Validation accuracy: 74.44%\n",
      "Epoch 460 of 1000. Training loss: 0.24, Validation accuracy: 74.44%\n",
      "Epoch 461 of 1000. Training loss: 0.22, Validation accuracy: 71.85%\n",
      "Epoch 462 of 1000. Training loss: 0.25, Validation accuracy: 75.19%\n",
      "Epoch 463 of 1000. Training loss: 0.24, Validation accuracy: 73.70%\n",
      "Epoch 464 of 1000. Training loss: 0.26, Validation accuracy: 74.07%\n",
      "Epoch 465 of 1000. Training loss: 0.22, Validation accuracy: 74.44%\n",
      "Epoch 466 of 1000. Training loss: 0.22, Validation accuracy: 73.70%\n",
      "Epoch 467 of 1000. Training loss: 0.21, Validation accuracy: 75.56%\n",
      "Epoch 468 of 1000. Training loss: 0.21, Validation accuracy: 73.33%\n",
      "Epoch 469 of 1000. Training loss: 0.22, Validation accuracy: 72.96%\n",
      "Epoch 470 of 1000. Training loss: 0.20, Validation accuracy: 73.33%\n",
      "Epoch 471 of 1000. Training loss: 0.20, Validation accuracy: 73.70%\n",
      "Epoch 472 of 1000. Training loss: 0.20, Validation accuracy: 73.70%\n",
      "Epoch 473 of 1000. Training loss: 0.21, Validation accuracy: 75.19%\n",
      "Epoch 474 of 1000. Training loss: 0.20, Validation accuracy: 72.22%\n",
      "Epoch 475 of 1000. Training loss: 0.22, Validation accuracy: 76.67%\n",
      "Epoch 476 of 1000. Training loss: 0.20, Validation accuracy: 77.41%\n",
      "Epoch 477 of 1000. Training loss: 0.18, Validation accuracy: 77.04%\n",
      "Epoch 478 of 1000. Training loss: 0.19, Validation accuracy: 74.44%\n",
      "Epoch 479 of 1000. Training loss: 0.19, Validation accuracy: 75.19%\n",
      "Epoch 480 of 1000. Training loss: 0.18, Validation accuracy: 75.19%\n",
      "Epoch 481 of 1000. Training loss: 0.22, Validation accuracy: 78.15%\n",
      "Epoch 482 of 1000. Training loss: 0.21, Validation accuracy: 74.07%\n",
      "Epoch 483 of 1000. Training loss: 0.19, Validation accuracy: 77.78%\n",
      "Epoch 484 of 1000. Training loss: 0.17, Validation accuracy: 72.22%\n",
      "Epoch 485 of 1000. Training loss: 0.19, Validation accuracy: 77.04%\n",
      "Epoch 486 of 1000. Training loss: 0.18, Validation accuracy: 77.04%\n",
      "Epoch 487 of 1000. Training loss: 0.19, Validation accuracy: 76.30%\n",
      "Epoch 488 of 1000. Training loss: 0.19, Validation accuracy: 74.07%\n",
      "Epoch 489 of 1000. Training loss: 0.20, Validation accuracy: 74.07%\n",
      "Epoch 490 of 1000. Training loss: 0.22, Validation accuracy: 73.33%\n",
      "Epoch 491 of 1000. Training loss: 0.19, Validation accuracy: 75.56%\n",
      "Epoch 492 of 1000. Training loss: 0.21, Validation accuracy: 76.67%\n",
      "Epoch 493 of 1000. Training loss: 0.21, Validation accuracy: 76.30%\n",
      "Epoch 494 of 1000. Training loss: 0.19, Validation accuracy: 75.93%\n",
      "Epoch 495 of 1000. Training loss: 0.17, Validation accuracy: 75.19%\n",
      "Epoch 496 of 1000. Training loss: 0.18, Validation accuracy: 76.30%\n",
      "Epoch 497 of 1000. Training loss: 0.17, Validation accuracy: 76.30%\n",
      "Epoch 498 of 1000. Training loss: 0.18, Validation accuracy: 73.70%\n",
      "Epoch 499 of 1000. Training loss: 0.16, Validation accuracy: 76.30%\n",
      "Epoch 500 of 1000. Training loss: 0.16, Validation accuracy: 73.70%\n",
      "Epoch 501 of 1000. Training loss: 0.19, Validation accuracy: 72.96%\n",
      "Epoch 502 of 1000. Training loss: 0.19, Validation accuracy: 74.81%\n",
      "Epoch 503 of 1000. Training loss: 0.18, Validation accuracy: 75.56%\n",
      "Epoch 504 of 1000. Training loss: 0.17, Validation accuracy: 75.93%\n",
      "Epoch 505 of 1000. Training loss: 0.17, Validation accuracy: 74.07%\n",
      "Epoch 506 of 1000. Training loss: 0.17, Validation accuracy: 74.81%\n",
      "Epoch 507 of 1000. Training loss: 0.21, Validation accuracy: 77.04%\n",
      "Epoch 508 of 1000. Training loss: 0.18, Validation accuracy: 74.44%\n",
      "Epoch 509 of 1000. Training loss: 0.17, Validation accuracy: 78.15%\n",
      "Epoch 510 of 1000. Training loss: 0.14, Validation accuracy: 76.67%\n",
      "Epoch 511 of 1000. Training loss: 0.15, Validation accuracy: 75.19%\n",
      "Epoch 512 of 1000. Training loss: 0.16, Validation accuracy: 74.07%\n",
      "Epoch 513 of 1000. Training loss: 0.16, Validation accuracy: 75.93%\n",
      "Epoch 514 of 1000. Training loss: 0.16, Validation accuracy: 75.56%\n",
      "Epoch 515 of 1000. Training loss: 0.17, Validation accuracy: 74.81%\n",
      "Epoch 516 of 1000. Training loss: 0.17, Validation accuracy: 76.30%\n",
      "Epoch 517 of 1000. Training loss: 0.17, Validation accuracy: 74.81%\n",
      "Epoch 518 of 1000. Training loss: 0.15, Validation accuracy: 75.56%\n",
      "Epoch 519 of 1000. Training loss: 0.16, Validation accuracy: 74.44%\n",
      "Epoch 520 of 1000. Training loss: 0.17, Validation accuracy: 74.44%\n",
      "Epoch 521 of 1000. Training loss: 0.18, Validation accuracy: 76.67%\n",
      "Epoch 522 of 1000. Training loss: 0.15, Validation accuracy: 73.70%\n",
      "Epoch 523 of 1000. Training loss: 0.16, Validation accuracy: 74.44%\n",
      "Epoch 524 of 1000. Training loss: 0.17, Validation accuracy: 74.81%\n",
      "Epoch 525 of 1000. Training loss: 0.19, Validation accuracy: 71.85%\n",
      "Epoch 526 of 1000. Training loss: 0.16, Validation accuracy: 71.11%\n",
      "Epoch 527 of 1000. Training loss: 0.15, Validation accuracy: 74.07%\n",
      "Epoch 528 of 1000. Training loss: 0.15, Validation accuracy: 72.96%\n",
      "Epoch 529 of 1000. Training loss: 0.18, Validation accuracy: 72.96%\n",
      "Epoch 530 of 1000. Training loss: 0.15, Validation accuracy: 75.93%\n",
      "Epoch 531 of 1000. Training loss: 0.15, Validation accuracy: 73.70%\n",
      "Epoch 532 of 1000. Training loss: 0.16, Validation accuracy: 74.07%\n",
      "Epoch 533 of 1000. Training loss: 0.19, Validation accuracy: 75.93%\n",
      "Epoch 534 of 1000. Training loss: 0.16, Validation accuracy: 73.33%\n",
      "Epoch 535 of 1000. Training loss: 0.17, Validation accuracy: 73.33%\n",
      "Epoch 536 of 1000. Training loss: 0.18, Validation accuracy: 73.33%\n",
      "Epoch 537 of 1000. Training loss: 0.15, Validation accuracy: 74.07%\n",
      "Epoch 538 of 1000. Training loss: 0.14, Validation accuracy: 73.33%\n",
      "Epoch 539 of 1000. Training loss: 0.14, Validation accuracy: 76.67%\n",
      "Epoch 540 of 1000. Training loss: 0.13, Validation accuracy: 75.19%\n",
      "Epoch 541 of 1000. Training loss: 0.12, Validation accuracy: 73.70%\n",
      "Epoch 542 of 1000. Training loss: 0.12, Validation accuracy: 73.70%\n",
      "Epoch 543 of 1000. Training loss: 0.15, Validation accuracy: 72.22%\n",
      "Epoch 544 of 1000. Training loss: 0.15, Validation accuracy: 74.07%\n",
      "Epoch 545 of 1000. Training loss: 0.14, Validation accuracy: 75.56%\n",
      "Epoch 546 of 1000. Training loss: 0.14, Validation accuracy: 72.96%\n",
      "Epoch 547 of 1000. Training loss: 0.18, Validation accuracy: 73.33%\n",
      "Epoch 548 of 1000. Training loss: 0.18, Validation accuracy: 75.19%\n",
      "Epoch 549 of 1000. Training loss: 0.13, Validation accuracy: 72.96%\n",
      "Epoch 550 of 1000. Training loss: 0.12, Validation accuracy: 73.70%\n",
      "Epoch 551 of 1000. Training loss: 0.15, Validation accuracy: 72.59%\n",
      "Epoch 552 of 1000. Training loss: 0.13, Validation accuracy: 74.44%\n",
      "Epoch 553 of 1000. Training loss: 0.14, Validation accuracy: 74.07%\n",
      "Epoch 554 of 1000. Training loss: 0.17, Validation accuracy: 72.22%\n",
      "Epoch 555 of 1000. Training loss: 0.16, Validation accuracy: 74.07%\n",
      "Epoch 556 of 1000. Training loss: 0.14, Validation accuracy: 75.19%\n",
      "Epoch 557 of 1000. Training loss: 0.14, Validation accuracy: 75.56%\n",
      "Epoch 558 of 1000. Training loss: 0.15, Validation accuracy: 74.07%\n",
      "Epoch 559 of 1000. Training loss: 0.16, Validation accuracy: 74.07%\n",
      "Epoch 560 of 1000. Training loss: 0.16, Validation accuracy: 75.19%\n",
      "Epoch 561 of 1000. Training loss: 0.12, Validation accuracy: 74.07%\n",
      "Epoch 562 of 1000. Training loss: 0.13, Validation accuracy: 73.33%\n",
      "Epoch 563 of 1000. Training loss: 0.13, Validation accuracy: 74.44%\n",
      "Epoch 564 of 1000. Training loss: 0.17, Validation accuracy: 72.96%\n",
      "Epoch 565 of 1000. Training loss: 0.17, Validation accuracy: 76.30%\n",
      "Epoch 566 of 1000. Training loss: 0.12, Validation accuracy: 72.96%\n",
      "Epoch 567 of 1000. Training loss: 0.17, Validation accuracy: 75.93%\n",
      "Epoch 568 of 1000. Training loss: 0.16, Validation accuracy: 75.19%\n",
      "Epoch 569 of 1000. Training loss: 0.13, Validation accuracy: 73.33%\n",
      "Epoch 570 of 1000. Training loss: 0.13, Validation accuracy: 74.07%\n",
      "Epoch 571 of 1000. Training loss: 0.14, Validation accuracy: 73.33%\n",
      "Epoch 572 of 1000. Training loss: 0.14, Validation accuracy: 74.07%\n",
      "Epoch 573 of 1000. Training loss: 0.12, Validation accuracy: 74.81%\n",
      "Epoch 574 of 1000. Training loss: 0.14, Validation accuracy: 73.33%\n",
      "Epoch 575 of 1000. Training loss: 0.13, Validation accuracy: 72.59%\n",
      "Epoch 576 of 1000. Training loss: 0.12, Validation accuracy: 74.81%\n",
      "Epoch 577 of 1000. Training loss: 0.13, Validation accuracy: 72.59%\n",
      "Epoch 578 of 1000. Training loss: 0.13, Validation accuracy: 74.81%\n",
      "Epoch 579 of 1000. Training loss: 0.14, Validation accuracy: 72.59%\n",
      "Epoch 580 of 1000. Training loss: 0.17, Validation accuracy: 75.19%\n",
      "Epoch 581 of 1000. Training loss: 0.17, Validation accuracy: 69.26%\n",
      "Epoch 582 of 1000. Training loss: 0.15, Validation accuracy: 75.56%\n",
      "Epoch 583 of 1000. Training loss: 0.11, Validation accuracy: 74.07%\n",
      "Epoch 584 of 1000. Training loss: 0.12, Validation accuracy: 75.93%\n",
      "Epoch 585 of 1000. Training loss: 0.12, Validation accuracy: 72.96%\n",
      "Epoch 586 of 1000. Training loss: 0.13, Validation accuracy: 71.11%\n",
      "Epoch 587 of 1000. Training loss: 0.14, Validation accuracy: 75.19%\n",
      "Epoch 588 of 1000. Training loss: 0.14, Validation accuracy: 73.33%\n",
      "Epoch 589 of 1000. Training loss: 0.14, Validation accuracy: 75.93%\n",
      "Epoch 590 of 1000. Training loss: 0.11, Validation accuracy: 75.56%\n",
      "Epoch 591 of 1000. Training loss: 0.11, Validation accuracy: 73.70%\n",
      "Epoch 592 of 1000. Training loss: 0.13, Validation accuracy: 72.96%\n",
      "Epoch 593 of 1000. Training loss: 0.15, Validation accuracy: 74.07%\n",
      "Epoch 594 of 1000. Training loss: 0.13, Validation accuracy: 72.96%\n",
      "Epoch 595 of 1000. Training loss: 0.13, Validation accuracy: 74.07%\n",
      "Epoch 596 of 1000. Training loss: 0.13, Validation accuracy: 74.81%\n",
      "Epoch 597 of 1000. Training loss: 0.14, Validation accuracy: 75.56%\n",
      "Epoch 598 of 1000. Training loss: 0.12, Validation accuracy: 74.81%\n",
      "Epoch 599 of 1000. Training loss: 0.16, Validation accuracy: 72.22%\n",
      "Epoch 600 of 1000. Training loss: 0.13, Validation accuracy: 75.56%\n",
      "Epoch 601 of 1000. Training loss: 0.11, Validation accuracy: 74.07%\n",
      "Epoch 602 of 1000. Training loss: 0.10, Validation accuracy: 74.07%\n",
      "Epoch 603 of 1000. Training loss: 0.09, Validation accuracy: 74.07%\n",
      "Epoch 604 of 1000. Training loss: 0.09, Validation accuracy: 75.56%\n",
      "Epoch 605 of 1000. Training loss: 0.09, Validation accuracy: 72.96%\n",
      "Epoch 606 of 1000. Training loss: 0.09, Validation accuracy: 76.30%\n",
      "Epoch 607 of 1000. Training loss: 0.09, Validation accuracy: 72.96%\n",
      "Epoch 608 of 1000. Training loss: 0.09, Validation accuracy: 74.81%\n",
      "Epoch 609 of 1000. Training loss: 0.09, Validation accuracy: 75.56%\n",
      "Epoch 610 of 1000. Training loss: 0.11, Validation accuracy: 75.19%\n",
      "Epoch 611 of 1000. Training loss: 0.09, Validation accuracy: 73.70%\n",
      "Epoch 612 of 1000. Training loss: 0.08, Validation accuracy: 72.22%\n",
      "Epoch 613 of 1000. Training loss: 0.08, Validation accuracy: 72.59%\n",
      "Epoch 614 of 1000. Training loss: 0.09, Validation accuracy: 74.81%\n",
      "Epoch 615 of 1000. Training loss: 0.09, Validation accuracy: 74.07%\n",
      "Epoch 616 of 1000. Training loss: 0.08, Validation accuracy: 74.07%\n",
      "Epoch 617 of 1000. Training loss: 0.09, Validation accuracy: 73.33%\n",
      "Epoch 618 of 1000. Training loss: 0.09, Validation accuracy: 72.59%\n",
      "Epoch 619 of 1000. Training loss: 0.09, Validation accuracy: 73.70%\n",
      "Epoch 620 of 1000. Training loss: 0.08, Validation accuracy: 75.93%\n",
      "Epoch 621 of 1000. Training loss: 0.09, Validation accuracy: 71.48%\n",
      "Epoch 622 of 1000. Training loss: 0.09, Validation accuracy: 73.70%\n",
      "Epoch 623 of 1000. Training loss: 0.08, Validation accuracy: 71.85%\n",
      "Epoch 624 of 1000. Training loss: 0.08, Validation accuracy: 75.19%\n",
      "Epoch 625 of 1000. Training loss: 0.08, Validation accuracy: 71.85%\n",
      "Epoch 626 of 1000. Training loss: 0.08, Validation accuracy: 73.70%\n",
      "Epoch 627 of 1000. Training loss: 0.08, Validation accuracy: 74.81%\n",
      "Epoch 628 of 1000. Training loss: 0.08, Validation accuracy: 74.81%\n",
      "Epoch 629 of 1000. Training loss: 0.09, Validation accuracy: 74.44%\n",
      "Epoch 630 of 1000. Training loss: 0.08, Validation accuracy: 75.56%\n",
      "Epoch 631 of 1000. Training loss: 0.08, Validation accuracy: 75.19%\n",
      "Epoch 632 of 1000. Training loss: 0.08, Validation accuracy: 75.19%\n",
      "Epoch 633 of 1000. Training loss: 0.07, Validation accuracy: 73.70%\n",
      "Epoch 634 of 1000. Training loss: 0.07, Validation accuracy: 75.56%\n",
      "Epoch 635 of 1000. Training loss: 0.07, Validation accuracy: 75.56%\n",
      "Epoch 636 of 1000. Training loss: 0.07, Validation accuracy: 75.19%\n",
      "Epoch 637 of 1000. Training loss: 0.08, Validation accuracy: 76.30%\n",
      "Epoch 638 of 1000. Training loss: 0.08, Validation accuracy: 74.81%\n",
      "Epoch 639 of 1000. Training loss: 0.08, Validation accuracy: 75.19%\n",
      "Epoch 640 of 1000. Training loss: 0.08, Validation accuracy: 72.59%\n",
      "Epoch 641 of 1000. Training loss: 0.07, Validation accuracy: 74.07%\n",
      "Epoch 642 of 1000. Training loss: 0.07, Validation accuracy: 74.81%\n",
      "Epoch 643 of 1000. Training loss: 0.07, Validation accuracy: 73.70%\n",
      "Epoch 644 of 1000. Training loss: 0.07, Validation accuracy: 75.56%\n",
      "Epoch 645 of 1000. Training loss: 0.08, Validation accuracy: 75.56%\n",
      "Epoch 646 of 1000. Training loss: 0.07, Validation accuracy: 75.19%\n",
      "Epoch 647 of 1000. Training loss: 0.07, Validation accuracy: 75.93%\n",
      "Epoch 648 of 1000. Training loss: 0.07, Validation accuracy: 73.33%\n",
      "Epoch 649 of 1000. Training loss: 0.07, Validation accuracy: 73.33%\n",
      "Epoch 650 of 1000. Training loss: 0.08, Validation accuracy: 74.07%\n",
      "Epoch 651 of 1000. Training loss: 0.07, Validation accuracy: 74.07%\n",
      "Epoch 652 of 1000. Training loss: 0.07, Validation accuracy: 75.93%\n",
      "Epoch 653 of 1000. Training loss: 0.10, Validation accuracy: 74.07%\n",
      "Epoch 654 of 1000. Training loss: 0.08, Validation accuracy: 72.22%\n",
      "Epoch 655 of 1000. Training loss: 0.07, Validation accuracy: 73.70%\n",
      "Epoch 656 of 1000. Training loss: 0.07, Validation accuracy: 73.70%\n",
      "Epoch 657 of 1000. Training loss: 0.07, Validation accuracy: 72.96%\n",
      "Epoch 658 of 1000. Training loss: 0.07, Validation accuracy: 74.07%\n",
      "Epoch 659 of 1000. Training loss: 0.08, Validation accuracy: 73.33%\n",
      "Epoch 660 of 1000. Training loss: 0.08, Validation accuracy: 71.48%\n",
      "Epoch 661 of 1000. Training loss: 0.07, Validation accuracy: 74.81%\n",
      "Epoch 662 of 1000. Training loss: 0.08, Validation accuracy: 75.56%\n",
      "Epoch 663 of 1000. Training loss: 0.08, Validation accuracy: 71.48%\n",
      "Epoch 664 of 1000. Training loss: 0.08, Validation accuracy: 72.22%\n",
      "Epoch 665 of 1000. Training loss: 0.07, Validation accuracy: 73.70%\n",
      "Epoch 666 of 1000. Training loss: 0.06, Validation accuracy: 74.81%\n",
      "Epoch 667 of 1000. Training loss: 0.08, Validation accuracy: 74.81%\n",
      "Epoch 668 of 1000. Training loss: 0.07, Validation accuracy: 74.44%\n",
      "Epoch 669 of 1000. Training loss: 0.06, Validation accuracy: 73.70%\n",
      "Epoch 670 of 1000. Training loss: 0.07, Validation accuracy: 71.48%\n",
      "Epoch 671 of 1000. Training loss: 0.09, Validation accuracy: 74.07%\n",
      "Epoch 672 of 1000. Training loss: 0.08, Validation accuracy: 75.19%\n",
      "Epoch 673 of 1000. Training loss: 0.08, Validation accuracy: 74.81%\n",
      "Epoch 674 of 1000. Training loss: 0.08, Validation accuracy: 76.30%\n",
      "Epoch 675 of 1000. Training loss: 0.08, Validation accuracy: 72.22%\n",
      "Epoch 676 of 1000. Training loss: 0.07, Validation accuracy: 73.33%\n",
      "Epoch 677 of 1000. Training loss: 0.07, Validation accuracy: 74.07%\n",
      "Epoch 678 of 1000. Training loss: 0.07, Validation accuracy: 72.59%\n",
      "Epoch 679 of 1000. Training loss: 0.08, Validation accuracy: 72.59%\n",
      "Epoch 680 of 1000. Training loss: 0.06, Validation accuracy: 73.33%\n",
      "Epoch 681 of 1000. Training loss: 0.07, Validation accuracy: 73.33%\n",
      "Epoch 682 of 1000. Training loss: 0.07, Validation accuracy: 74.81%\n",
      "Epoch 683 of 1000. Training loss: 0.06, Validation accuracy: 73.33%\n",
      "Epoch 684 of 1000. Training loss: 0.09, Validation accuracy: 71.11%\n",
      "Epoch 685 of 1000. Training loss: 0.09, Validation accuracy: 70.74%\n",
      "Epoch 686 of 1000. Training loss: 0.06, Validation accuracy: 70.74%\n",
      "Epoch 687 of 1000. Training loss: 0.09, Validation accuracy: 72.59%\n",
      "Epoch 688 of 1000. Training loss: 0.09, Validation accuracy: 72.59%\n",
      "Epoch 689 of 1000. Training loss: 0.06, Validation accuracy: 72.22%\n",
      "Epoch 690 of 1000. Training loss: 0.06, Validation accuracy: 71.48%\n",
      "Epoch 691 of 1000. Training loss: 0.07, Validation accuracy: 74.07%\n",
      "Epoch 692 of 1000. Training loss: 0.07, Validation accuracy: 75.19%\n",
      "Epoch 693 of 1000. Training loss: 0.06, Validation accuracy: 74.81%\n",
      "Epoch 694 of 1000. Training loss: 0.06, Validation accuracy: 72.96%\n",
      "Epoch 695 of 1000. Training loss: 0.06, Validation accuracy: 72.96%\n",
      "Epoch 696 of 1000. Training loss: 0.08, Validation accuracy: 73.70%\n",
      "Epoch 697 of 1000. Training loss: 0.06, Validation accuracy: 72.22%\n",
      "Epoch 698 of 1000. Training loss: 0.07, Validation accuracy: 72.96%\n",
      "Epoch 699 of 1000. Training loss: 0.07, Validation accuracy: 73.33%\n",
      "Epoch 700 of 1000. Training loss: 0.06, Validation accuracy: 72.59%\n",
      "Epoch 701 of 1000. Training loss: 0.07, Validation accuracy: 74.81%\n",
      "Epoch 702 of 1000. Training loss: 0.06, Validation accuracy: 73.70%\n",
      "Epoch 703 of 1000. Training loss: 0.06, Validation accuracy: 75.19%\n",
      "Epoch 704 of 1000. Training loss: 0.08, Validation accuracy: 72.59%\n",
      "Epoch 705 of 1000. Training loss: 0.06, Validation accuracy: 72.96%\n",
      "Epoch 706 of 1000. Training loss: 0.05, Validation accuracy: 71.85%\n",
      "Epoch 707 of 1000. Training loss: 0.06, Validation accuracy: 70.00%\n",
      "Epoch 708 of 1000. Training loss: 0.07, Validation accuracy: 73.33%\n",
      "Epoch 709 of 1000. Training loss: 0.05, Validation accuracy: 72.22%\n",
      "Epoch 710 of 1000. Training loss: 0.06, Validation accuracy: 74.07%\n",
      "Epoch 711 of 1000. Training loss: 0.07, Validation accuracy: 72.96%\n",
      "Epoch 712 of 1000. Training loss: 0.06, Validation accuracy: 73.70%\n",
      "Epoch 713 of 1000. Training loss: 0.06, Validation accuracy: 71.48%\n",
      "Epoch 714 of 1000. Training loss: 0.05, Validation accuracy: 73.70%\n",
      "Epoch 715 of 1000. Training loss: 0.05, Validation accuracy: 73.33%\n",
      "Epoch 716 of 1000. Training loss: 0.06, Validation accuracy: 73.33%\n",
      "Epoch 717 of 1000. Training loss: 0.07, Validation accuracy: 72.22%\n",
      "Epoch 718 of 1000. Training loss: 0.07, Validation accuracy: 69.63%\n",
      "Epoch 719 of 1000. Training loss: 0.06, Validation accuracy: 71.85%\n",
      "Epoch 720 of 1000. Training loss: 0.06, Validation accuracy: 73.70%\n",
      "Epoch 721 of 1000. Training loss: 0.07, Validation accuracy: 72.22%\n",
      "Epoch 722 of 1000. Training loss: 0.05, Validation accuracy: 72.59%\n",
      "Epoch 723 of 1000. Training loss: 0.05, Validation accuracy: 72.96%\n",
      "Epoch 724 of 1000. Training loss: 0.05, Validation accuracy: 72.96%\n",
      "Epoch 725 of 1000. Training loss: 0.06, Validation accuracy: 73.33%\n",
      "Epoch 726 of 1000. Training loss: 0.07, Validation accuracy: 72.22%\n",
      "Epoch 727 of 1000. Training loss: 0.05, Validation accuracy: 72.59%\n",
      "Epoch 728 of 1000. Training loss: 0.05, Validation accuracy: 73.70%\n",
      "Epoch 729 of 1000. Training loss: 0.05, Validation accuracy: 71.11%\n",
      "Epoch 730 of 1000. Training loss: 0.06, Validation accuracy: 74.07%\n",
      "Epoch 731 of 1000. Training loss: 0.04, Validation accuracy: 73.33%\n",
      "Epoch 732 of 1000. Training loss: 0.05, Validation accuracy: 72.22%\n",
      "Epoch 733 of 1000. Training loss: 0.06, Validation accuracy: 72.96%\n",
      "Epoch 734 of 1000. Training loss: 0.06, Validation accuracy: 72.59%\n",
      "Epoch 735 of 1000. Training loss: 0.07, Validation accuracy: 72.59%\n",
      "Epoch 736 of 1000. Training loss: 0.05, Validation accuracy: 73.70%\n",
      "Epoch 737 of 1000. Training loss: 0.05, Validation accuracy: 74.07%\n",
      "Epoch 738 of 1000. Training loss: 0.06, Validation accuracy: 74.07%\n",
      "Epoch 739 of 1000. Training loss: 0.06, Validation accuracy: 71.85%\n",
      "Epoch 740 of 1000. Training loss: 0.06, Validation accuracy: 72.96%\n",
      "Epoch 741 of 1000. Training loss: 0.06, Validation accuracy: 71.85%\n",
      "Epoch 742 of 1000. Training loss: 0.06, Validation accuracy: 71.48%\n",
      "Epoch 743 of 1000. Training loss: 0.06, Validation accuracy: 74.44%\n",
      "Epoch 744 of 1000. Training loss: 0.05, Validation accuracy: 74.81%\n",
      "Epoch 745 of 1000. Training loss: 0.07, Validation accuracy: 73.33%\n",
      "Epoch 746 of 1000. Training loss: 0.05, Validation accuracy: 71.48%\n",
      "Epoch 747 of 1000. Training loss: 0.06, Validation accuracy: 73.70%\n",
      "Epoch 748 of 1000. Training loss: 0.06, Validation accuracy: 73.33%\n",
      "Epoch 749 of 1000. Training loss: 0.05, Validation accuracy: 74.44%\n",
      "Epoch 750 of 1000. Training loss: 0.05, Validation accuracy: 72.59%\n",
      "Epoch 751 of 1000. Training loss: 0.07, Validation accuracy: 71.11%\n",
      "Epoch 752 of 1000. Training loss: 0.05, Validation accuracy: 71.85%\n",
      "Epoch 753 of 1000. Training loss: 0.05, Validation accuracy: 71.85%\n",
      "Epoch 754 of 1000. Training loss: 0.05, Validation accuracy: 74.44%\n",
      "Epoch 755 of 1000. Training loss: 0.05, Validation accuracy: 74.07%\n",
      "Epoch 756 of 1000. Training loss: 0.05, Validation accuracy: 71.85%\n",
      "Epoch 757 of 1000. Training loss: 0.06, Validation accuracy: 72.96%\n",
      "Epoch 758 of 1000. Training loss: 0.06, Validation accuracy: 71.48%\n",
      "Epoch 759 of 1000. Training loss: 0.05, Validation accuracy: 71.11%\n",
      "Epoch 760 of 1000. Training loss: 0.05, Validation accuracy: 71.48%\n",
      "Epoch 761 of 1000. Training loss: 0.05, Validation accuracy: 74.07%\n",
      "Epoch 762 of 1000. Training loss: 0.05, Validation accuracy: 73.70%\n",
      "Epoch 763 of 1000. Training loss: 0.05, Validation accuracy: 72.59%\n",
      "Epoch 764 of 1000. Training loss: 0.04, Validation accuracy: 70.00%\n",
      "Epoch 765 of 1000. Training loss: 0.05, Validation accuracy: 71.85%\n",
      "Epoch 766 of 1000. Training loss: 0.06, Validation accuracy: 71.48%\n",
      "Epoch 767 of 1000. Training loss: 0.06, Validation accuracy: 71.85%\n",
      "Epoch 768 of 1000. Training loss: 0.05, Validation accuracy: 72.59%\n",
      "Epoch 769 of 1000. Training loss: 0.04, Validation accuracy: 72.96%\n",
      "Epoch 770 of 1000. Training loss: 0.07, Validation accuracy: 72.96%\n",
      "Epoch 771 of 1000. Training loss: 0.05, Validation accuracy: 72.59%\n",
      "Epoch 772 of 1000. Training loss: 0.04, Validation accuracy: 72.22%\n",
      "Epoch 773 of 1000. Training loss: 0.05, Validation accuracy: 70.74%\n",
      "Epoch 774 of 1000. Training loss: 0.06, Validation accuracy: 72.22%\n",
      "Epoch 775 of 1000. Training loss: 0.06, Validation accuracy: 70.74%\n",
      "Epoch 776 of 1000. Training loss: 0.05, Validation accuracy: 70.74%\n",
      "Epoch 777 of 1000. Training loss: 0.05, Validation accuracy: 72.59%\n",
      "Epoch 778 of 1000. Training loss: 0.05, Validation accuracy: 70.37%\n",
      "Epoch 779 of 1000. Training loss: 0.07, Validation accuracy: 73.70%\n",
      "Epoch 780 of 1000. Training loss: 0.06, Validation accuracy: 69.63%\n",
      "Epoch 781 of 1000. Training loss: 0.06, Validation accuracy: 70.37%\n",
      "Epoch 782 of 1000. Training loss: 0.06, Validation accuracy: 70.74%\n",
      "Epoch 783 of 1000. Training loss: 0.05, Validation accuracy: 72.22%\n",
      "Epoch 784 of 1000. Training loss: 0.04, Validation accuracy: 70.74%\n",
      "Epoch 785 of 1000. Training loss: 0.05, Validation accuracy: 74.81%\n",
      "Epoch 786 of 1000. Training loss: 0.06, Validation accuracy: 71.48%\n",
      "Epoch 787 of 1000. Training loss: 0.05, Validation accuracy: 72.59%\n",
      "Epoch 788 of 1000. Training loss: 0.09, Validation accuracy: 70.74%\n",
      "Epoch 789 of 1000. Training loss: 0.09, Validation accuracy: 74.44%\n",
      "Epoch 790 of 1000. Training loss: 0.05, Validation accuracy: 73.33%\n",
      "Epoch 791 of 1000. Training loss: 0.04, Validation accuracy: 72.22%\n",
      "Epoch 792 of 1000. Training loss: 0.04, Validation accuracy: 71.48%\n",
      "Epoch 793 of 1000. Training loss: 0.06, Validation accuracy: 73.33%\n",
      "Epoch 794 of 1000. Training loss: 0.05, Validation accuracy: 72.96%\n",
      "Epoch 795 of 1000. Training loss: 0.06, Validation accuracy: 72.22%\n",
      "Epoch 796 of 1000. Training loss: 0.06, Validation accuracy: 70.74%\n",
      "Epoch 797 of 1000. Training loss: 0.06, Validation accuracy: 69.63%\n",
      "Epoch 798 of 1000. Training loss: 0.05, Validation accuracy: 71.85%\n",
      "Epoch 799 of 1000. Training loss: 0.04, Validation accuracy: 72.22%\n",
      "Epoch 800 of 1000. Training loss: 0.05, Validation accuracy: 72.96%\n",
      "Epoch 801 of 1000. Training loss: 0.05, Validation accuracy: 73.33%\n",
      "Epoch 802 of 1000. Training loss: 0.04, Validation accuracy: 71.48%\n",
      "Epoch 803 of 1000. Training loss: 0.03, Validation accuracy: 70.74%\n",
      "Epoch 804 of 1000. Training loss: 0.03, Validation accuracy: 71.48%\n",
      "Epoch 805 of 1000. Training loss: 0.03, Validation accuracy: 71.11%\n",
      "Epoch 806 of 1000. Training loss: 0.03, Validation accuracy: 71.11%\n",
      "Epoch 807 of 1000. Training loss: 0.03, Validation accuracy: 71.85%\n",
      "Epoch 808 of 1000. Training loss: 0.03, Validation accuracy: 71.85%\n",
      "Epoch 809 of 1000. Training loss: 0.03, Validation accuracy: 71.85%\n",
      "Epoch 810 of 1000. Training loss: 0.03, Validation accuracy: 71.85%\n",
      "Epoch 811 of 1000. Training loss: 0.03, Validation accuracy: 71.85%\n",
      "Epoch 812 of 1000. Training loss: 0.03, Validation accuracy: 70.74%\n",
      "Epoch 813 of 1000. Training loss: 0.03, Validation accuracy: 71.48%\n",
      "Epoch 814 of 1000. Training loss: 0.03, Validation accuracy: 71.11%\n",
      "Epoch 815 of 1000. Training loss: 0.03, Validation accuracy: 70.74%\n",
      "Epoch 816 of 1000. Training loss: 0.03, Validation accuracy: 71.48%\n",
      "Epoch 817 of 1000. Training loss: 0.03, Validation accuracy: 70.74%\n",
      "Epoch 818 of 1000. Training loss: 0.03, Validation accuracy: 71.48%\n",
      "Epoch 819 of 1000. Training loss: 0.03, Validation accuracy: 71.85%\n",
      "Epoch 820 of 1000. Training loss: 0.03, Validation accuracy: 71.48%\n",
      "Epoch 821 of 1000. Training loss: 0.03, Validation accuracy: 71.48%\n",
      "Epoch 822 of 1000. Training loss: 0.03, Validation accuracy: 70.74%\n",
      "Epoch 823 of 1000. Training loss: 0.03, Validation accuracy: 71.85%\n",
      "Epoch 824 of 1000. Training loss: 0.03, Validation accuracy: 71.11%\n",
      "Epoch 825 of 1000. Training loss: 0.03, Validation accuracy: 71.48%\n",
      "Epoch 826 of 1000. Training loss: 0.03, Validation accuracy: 71.11%\n",
      "Epoch 827 of 1000. Training loss: 0.03, Validation accuracy: 71.11%\n",
      "Epoch 828 of 1000. Training loss: 0.03, Validation accuracy: 71.48%\n",
      "Epoch 829 of 1000. Training loss: 0.03, Validation accuracy: 71.11%\n",
      "Epoch 830 of 1000. Training loss: 0.03, Validation accuracy: 71.11%\n",
      "Epoch 831 of 1000. Training loss: 0.03, Validation accuracy: 70.37%\n",
      "Epoch 832 of 1000. Training loss: 0.03, Validation accuracy: 71.11%\n",
      "Epoch 833 of 1000. Training loss: 0.03, Validation accuracy: 72.59%\n",
      "Epoch 834 of 1000. Training loss: 0.03, Validation accuracy: 71.11%\n",
      "Epoch 835 of 1000. Training loss: 0.03, Validation accuracy: 71.11%\n",
      "Epoch 836 of 1000. Training loss: 0.03, Validation accuracy: 71.11%\n",
      "Epoch 837 of 1000. Training loss: 0.03, Validation accuracy: 71.11%\n",
      "Epoch 838 of 1000. Training loss: 0.03, Validation accuracy: 71.48%\n",
      "Epoch 839 of 1000. Training loss: 0.03, Validation accuracy: 71.11%\n",
      "Epoch 840 of 1000. Training loss: 0.03, Validation accuracy: 72.22%\n",
      "Epoch 841 of 1000. Training loss: 0.03, Validation accuracy: 71.85%\n",
      "Epoch 842 of 1000. Training loss: 0.03, Validation accuracy: 71.85%\n",
      "Epoch 843 of 1000. Training loss: 0.03, Validation accuracy: 71.85%\n",
      "Epoch 844 of 1000. Training loss: 0.03, Validation accuracy: 72.22%\n",
      "Epoch 845 of 1000. Training loss: 0.03, Validation accuracy: 71.85%\n",
      "Epoch 846 of 1000. Training loss: 0.03, Validation accuracy: 72.22%\n",
      "Epoch 847 of 1000. Training loss: 0.03, Validation accuracy: 72.59%\n",
      "Epoch 848 of 1000. Training loss: 0.03, Validation accuracy: 71.85%\n",
      "Epoch 849 of 1000. Training loss: 0.03, Validation accuracy: 72.22%\n",
      "Epoch 850 of 1000. Training loss: 0.03, Validation accuracy: 71.85%\n",
      "Epoch 851 of 1000. Training loss: 0.03, Validation accuracy: 71.48%\n",
      "Epoch 852 of 1000. Training loss: 0.02, Validation accuracy: 71.11%\n",
      "Epoch 853 of 1000. Training loss: 0.03, Validation accuracy: 70.37%\n",
      "Epoch 854 of 1000. Training loss: 0.03, Validation accuracy: 71.48%\n",
      "Epoch 855 of 1000. Training loss: 0.02, Validation accuracy: 71.85%\n",
      "Epoch 856 of 1000. Training loss: 0.02, Validation accuracy: 71.85%\n",
      "Epoch 857 of 1000. Training loss: 0.02, Validation accuracy: 71.85%\n",
      "Epoch 858 of 1000. Training loss: 0.02, Validation accuracy: 72.22%\n",
      "Epoch 859 of 1000. Training loss: 0.03, Validation accuracy: 71.85%\n",
      "Epoch 860 of 1000. Training loss: 0.02, Validation accuracy: 70.74%\n",
      "Epoch 861 of 1000. Training loss: 0.02, Validation accuracy: 71.85%\n",
      "Epoch 862 of 1000. Training loss: 0.02, Validation accuracy: 71.11%\n",
      "Epoch 863 of 1000. Training loss: 0.02, Validation accuracy: 70.00%\n",
      "Epoch 864 of 1000. Training loss: 0.02, Validation accuracy: 71.11%\n",
      "Epoch 865 of 1000. Training loss: 0.02, Validation accuracy: 71.48%\n",
      "Epoch 866 of 1000. Training loss: 0.02, Validation accuracy: 70.74%\n",
      "Epoch 867 of 1000. Training loss: 0.02, Validation accuracy: 71.48%\n",
      "Epoch 868 of 1000. Training loss: 0.03, Validation accuracy: 70.74%\n",
      "Epoch 869 of 1000. Training loss: 0.02, Validation accuracy: 71.85%\n",
      "Epoch 870 of 1000. Training loss: 0.02, Validation accuracy: 72.22%\n",
      "Epoch 871 of 1000. Training loss: 0.02, Validation accuracy: 72.22%\n",
      "Epoch 872 of 1000. Training loss: 0.02, Validation accuracy: 71.11%\n",
      "Epoch 873 of 1000. Training loss: 0.02, Validation accuracy: 72.96%\n",
      "Epoch 874 of 1000. Training loss: 0.02, Validation accuracy: 71.48%\n",
      "Epoch 875 of 1000. Training loss: 0.02, Validation accuracy: 71.85%\n",
      "Epoch 876 of 1000. Training loss: 0.02, Validation accuracy: 71.85%\n",
      "Epoch 877 of 1000. Training loss: 0.02, Validation accuracy: 71.85%\n",
      "Epoch 878 of 1000. Training loss: 0.02, Validation accuracy: 71.85%\n",
      "Epoch 879 of 1000. Training loss: 0.02, Validation accuracy: 72.96%\n",
      "Epoch 880 of 1000. Training loss: 0.02, Validation accuracy: 70.74%\n",
      "Epoch 881 of 1000. Training loss: 0.02, Validation accuracy: 70.74%\n",
      "Epoch 882 of 1000. Training loss: 0.02, Validation accuracy: 70.37%\n",
      "Epoch 883 of 1000. Training loss: 0.02, Validation accuracy: 71.85%\n",
      "Epoch 884 of 1000. Training loss: 0.02, Validation accuracy: 70.74%\n",
      "Epoch 885 of 1000. Training loss: 0.03, Validation accuracy: 71.48%\n",
      "Epoch 886 of 1000. Training loss: 0.02, Validation accuracy: 71.11%\n",
      "Epoch 887 of 1000. Training loss: 0.02, Validation accuracy: 71.11%\n",
      "Epoch 888 of 1000. Training loss: 0.02, Validation accuracy: 70.37%\n",
      "Epoch 889 of 1000. Training loss: 0.02, Validation accuracy: 71.85%\n",
      "Epoch 890 of 1000. Training loss: 0.02, Validation accuracy: 70.37%\n",
      "Epoch 891 of 1000. Training loss: 0.03, Validation accuracy: 70.74%\n",
      "Epoch 892 of 1000. Training loss: 0.02, Validation accuracy: 71.48%\n",
      "Epoch 893 of 1000. Training loss: 0.03, Validation accuracy: 72.22%\n",
      "Epoch 894 of 1000. Training loss: 0.02, Validation accuracy: 70.37%\n",
      "Epoch 895 of 1000. Training loss: 0.02, Validation accuracy: 70.37%\n",
      "Epoch 896 of 1000. Training loss: 0.02, Validation accuracy: 69.63%\n",
      "Epoch 897 of 1000. Training loss: 0.02, Validation accuracy: 71.11%\n",
      "Epoch 898 of 1000. Training loss: 0.02, Validation accuracy: 71.11%\n",
      "Epoch 899 of 1000. Training loss: 0.02, Validation accuracy: 71.11%\n",
      "Epoch 900 of 1000. Training loss: 0.02, Validation accuracy: 71.11%\n",
      "Epoch 901 of 1000. Training loss: 0.05, Validation accuracy: 71.85%\n",
      "Epoch 902 of 1000. Training loss: 0.04, Validation accuracy: 70.37%\n",
      "Epoch 903 of 1000. Training loss: 0.02, Validation accuracy: 70.00%\n",
      "Epoch 904 of 1000. Training loss: 0.02, Validation accuracy: 70.37%\n",
      "Epoch 905 of 1000. Training loss: 0.02, Validation accuracy: 70.00%\n",
      "Epoch 906 of 1000. Training loss: 0.02, Validation accuracy: 69.63%\n",
      "Epoch 907 of 1000. Training loss: 0.02, Validation accuracy: 71.11%\n",
      "Epoch 908 of 1000. Training loss: 0.02, Validation accuracy: 71.48%\n",
      "Epoch 909 of 1000. Training loss: 0.02, Validation accuracy: 70.37%\n",
      "Epoch 910 of 1000. Training loss: 0.02, Validation accuracy: 70.00%\n",
      "Epoch 911 of 1000. Training loss: 0.02, Validation accuracy: 71.11%\n",
      "Epoch 912 of 1000. Training loss: 0.02, Validation accuracy: 70.74%\n",
      "Epoch 913 of 1000. Training loss: 0.02, Validation accuracy: 70.37%\n",
      "Epoch 914 of 1000. Training loss: 0.03, Validation accuracy: 72.22%\n",
      "Epoch 915 of 1000. Training loss: 0.03, Validation accuracy: 68.15%\n",
      "Epoch 916 of 1000. Training loss: 0.02, Validation accuracy: 70.00%\n",
      "Epoch 917 of 1000. Training loss: 0.02, Validation accuracy: 70.00%\n",
      "Epoch 918 of 1000. Training loss: 0.02, Validation accuracy: 70.00%\n",
      "Epoch 919 of 1000. Training loss: 0.02, Validation accuracy: 70.37%\n",
      "Epoch 920 of 1000. Training loss: 0.02, Validation accuracy: 71.48%\n",
      "Epoch 921 of 1000. Training loss: 0.03, Validation accuracy: 71.85%\n",
      "Epoch 922 of 1000. Training loss: 0.02, Validation accuracy: 72.22%\n",
      "Epoch 923 of 1000. Training loss: 0.02, Validation accuracy: 70.37%\n",
      "Epoch 924 of 1000. Training loss: 0.02, Validation accuracy: 70.37%\n",
      "Epoch 925 of 1000. Training loss: 0.02, Validation accuracy: 70.00%\n",
      "Epoch 926 of 1000. Training loss: 0.02, Validation accuracy: 70.74%\n",
      "Epoch 927 of 1000. Training loss: 0.02, Validation accuracy: 70.74%\n",
      "Epoch 928 of 1000. Training loss: 0.02, Validation accuracy: 70.74%\n",
      "Epoch 929 of 1000. Training loss: 0.02, Validation accuracy: 71.11%\n",
      "Epoch 930 of 1000. Training loss: 0.02, Validation accuracy: 71.11%\n",
      "Epoch 931 of 1000. Training loss: 0.02, Validation accuracy: 72.22%\n",
      "Epoch 932 of 1000. Training loss: 0.02, Validation accuracy: 72.22%\n",
      "Epoch 933 of 1000. Training loss: 0.02, Validation accuracy: 71.48%\n",
      "Epoch 934 of 1000. Training loss: 0.02, Validation accuracy: 70.74%\n",
      "Epoch 935 of 1000. Training loss: 0.02, Validation accuracy: 71.11%\n",
      "Epoch 936 of 1000. Training loss: 0.02, Validation accuracy: 71.11%\n",
      "Epoch 937 of 1000. Training loss: 0.02, Validation accuracy: 71.11%\n",
      "Epoch 938 of 1000. Training loss: 0.02, Validation accuracy: 71.11%\n",
      "Epoch 939 of 1000. Training loss: 0.02, Validation accuracy: 70.74%\n",
      "Epoch 940 of 1000. Training loss: 0.02, Validation accuracy: 71.48%\n",
      "Epoch 941 of 1000. Training loss: 0.02, Validation accuracy: 70.37%\n",
      "Epoch 942 of 1000. Training loss: 0.02, Validation accuracy: 70.37%\n",
      "Epoch 943 of 1000. Training loss: 0.02, Validation accuracy: 71.48%\n",
      "Epoch 944 of 1000. Training loss: 0.03, Validation accuracy: 71.48%\n",
      "Epoch 945 of 1000. Training loss: 0.04, Validation accuracy: 72.22%\n",
      "Epoch 946 of 1000. Training loss: 0.02, Validation accuracy: 72.59%\n",
      "Epoch 947 of 1000. Training loss: 0.02, Validation accuracy: 71.48%\n",
      "Epoch 948 of 1000. Training loss: 0.02, Validation accuracy: 71.48%\n",
      "Epoch 949 of 1000. Training loss: 0.02, Validation accuracy: 71.11%\n",
      "Epoch 950 of 1000. Training loss: 0.02, Validation accuracy: 70.74%\n",
      "Epoch 951 of 1000. Training loss: 0.02, Validation accuracy: 70.37%\n",
      "Epoch 952 of 1000. Training loss: 0.02, Validation accuracy: 71.85%\n",
      "Epoch 953 of 1000. Training loss: 0.02, Validation accuracy: 71.11%\n",
      "Epoch 954 of 1000. Training loss: 0.02, Validation accuracy: 72.59%\n",
      "Epoch 955 of 1000. Training loss: 0.02, Validation accuracy: 71.85%\n",
      "Epoch 956 of 1000. Training loss: 0.02, Validation accuracy: 70.74%\n",
      "Epoch 957 of 1000. Training loss: 0.02, Validation accuracy: 71.48%\n",
      "Epoch 958 of 1000. Training loss: 0.02, Validation accuracy: 71.85%\n",
      "Epoch 959 of 1000. Training loss: 0.02, Validation accuracy: 71.11%\n",
      "Epoch 960 of 1000. Training loss: 0.02, Validation accuracy: 72.22%\n",
      "Epoch 961 of 1000. Training loss: 0.02, Validation accuracy: 73.33%\n",
      "Epoch 962 of 1000. Training loss: 0.02, Validation accuracy: 72.59%\n",
      "Epoch 963 of 1000. Training loss: 0.02, Validation accuracy: 72.59%\n",
      "Epoch 964 of 1000. Training loss: 0.02, Validation accuracy: 73.33%\n",
      "Epoch 965 of 1000. Training loss: 0.03, Validation accuracy: 72.96%\n",
      "Epoch 966 of 1000. Training loss: 0.02, Validation accuracy: 72.22%\n",
      "Epoch 967 of 1000. Training loss: 0.02, Validation accuracy: 72.96%\n",
      "Epoch 968 of 1000. Training loss: 0.02, Validation accuracy: 73.33%\n",
      "Epoch 969 of 1000. Training loss: 0.02, Validation accuracy: 73.33%\n",
      "Epoch 970 of 1000. Training loss: 0.02, Validation accuracy: 73.33%\n",
      "Epoch 971 of 1000. Training loss: 0.02, Validation accuracy: 71.85%\n",
      "Epoch 972 of 1000. Training loss: 0.02, Validation accuracy: 73.33%\n",
      "Epoch 973 of 1000. Training loss: 0.02, Validation accuracy: 73.33%\n",
      "Epoch 974 of 1000. Training loss: 0.02, Validation accuracy: 72.59%\n",
      "Epoch 975 of 1000. Training loss: 0.02, Validation accuracy: 74.07%\n",
      "Epoch 976 of 1000. Training loss: 0.02, Validation accuracy: 73.33%\n",
      "Epoch 977 of 1000. Training loss: 0.02, Validation accuracy: 72.59%\n",
      "Epoch 978 of 1000. Training loss: 0.02, Validation accuracy: 72.59%\n",
      "Epoch 979 of 1000. Training loss: 0.02, Validation accuracy: 73.70%\n",
      "Epoch 980 of 1000. Training loss: 0.03, Validation accuracy: 73.33%\n",
      "Epoch 981 of 1000. Training loss: 0.02, Validation accuracy: 72.59%\n",
      "Epoch 982 of 1000. Training loss: 0.02, Validation accuracy: 71.85%\n",
      "Epoch 983 of 1000. Training loss: 0.02, Validation accuracy: 72.59%\n",
      "Epoch 984 of 1000. Training loss: 0.02, Validation accuracy: 73.33%\n",
      "Epoch 985 of 1000. Training loss: 0.02, Validation accuracy: 72.59%\n",
      "Epoch 986 of 1000. Training loss: 0.02, Validation accuracy: 72.59%\n",
      "Epoch 987 of 1000. Training loss: 0.02, Validation accuracy: 72.96%\n",
      "Epoch 988 of 1000. Training loss: 0.02, Validation accuracy: 71.85%\n",
      "Epoch 989 of 1000. Training loss: 0.02, Validation accuracy: 72.59%\n",
      "Epoch 990 of 1000. Training loss: 0.02, Validation accuracy: 72.96%\n",
      "Epoch 991 of 1000. Training loss: 0.02, Validation accuracy: 73.33%\n",
      "Epoch 992 of 1000. Training loss: 0.02, Validation accuracy: 72.22%\n",
      "Epoch 993 of 1000. Training loss: 0.02, Validation accuracy: 73.33%\n",
      "Epoch 994 of 1000. Training loss: 0.02, Validation accuracy: 72.96%\n",
      "Epoch 995 of 1000. Training loss: 0.02, Validation accuracy: 71.85%\n",
      "Epoch 996 of 1000. Training loss: 0.02, Validation accuracy: 71.11%\n",
      "Epoch 997 of 1000. Training loss: 0.02, Validation accuracy: 71.11%\n",
      "Epoch 998 of 1000. Training loss: 0.02, Validation accuracy: 72.59%\n",
      "Epoch 999 of 1000. Training loss: 0.02, Validation accuracy: 73.33%\n",
      "Epoch 1000 of 1000. Training loss: 0.02, Validation accuracy: 72.59%\n"
     ]
    }
   ],
   "source": [
    "# QAT\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "## Training Setup\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=args['learning_rate'])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "## Training Loop\n",
    "for ep in range(1,args['epochs']+1):\n",
    "    #Do backpropgation and validation epochs\n",
    "    loss = train_model(trainset,trainlabels,model,optimizer,criterion,**args)\n",
    "    scheduler.step()\n",
    "    acc = validate_model(validset,validlabels,model,**args)\n",
    "    print('Epoch {0:d} of {1:d}. Training loss: {2:.2f}, Validation accuracy: {3:.2f}%'.format(ep,args['epochs'],loss,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'chkpt_t2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.66666666666667"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = validate_model(testset,testlabels,model,**args)\n",
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
